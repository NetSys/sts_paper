Our goal is to transform
event traces (`test cases') into minimal sequences of inputs.
The closest work to us is delta
debugging~\cite{Zeller:1999:YMP:318773.318946,Zeller:2002:SIF:506201.506206}
and other domain specific variants such as Chronus~\cite{whitaker2004configuration},
which seek to minimize test cases to individual programs. As far as we know we
are the first to apply delta debugging to a distributed system, where test
cases are injected over a period of time rather than at once, and involve
causal relationships between multiple actors. The authors of delta debugging
have applied their technique to multi-threaded % single-core
programs to minimize thread schedules that lead to race conditions, but this still
involves injecting a single input (the schedule) to a single entity (the
scheduler)~\cite{choi2002isolating}.

We characterize the other approaches taken by the troubleshooting literature
as (i) instrumentation,
(ii) bug detection, (iii) replay, and (iv) root cause analysis.

\noindent{\bf Instrumentation.} Unstructured
log files collected at each node are the most common form of diagnostic information. The goal of
tracing
frameworks~\cite{pip,fonseca2007x,Chen02pinpoint:problem,ndb14,barham2004using} 
is to produce structured logs that can be easily analyzed, such as DAGs tracking the events triggered by
requests passing through the distributed system. An example within the SDN
problem space is ndb~\cite{ndb14}, which
allows users to retroactively examine traces of the OpenFlow forwarding rules
encountered by dataplane packets. Our goal is to minimize such
traces.

\noindent{\bf Bug Detection.} With instrumentation available, it becomes possible
to check expectations about the
the system's state (either offline~\cite{Liu07widschecker} or online~\cite{d3s,dao2009live}), or about the paths requests take through
the system~\cite{pip}. Within the field of networking, this research is
primarily focused on verifying routing tables~\cite{hsa,hsa_realtime,anteater,khurshid2012veriflow}
or forwarding behavior~\cite{Zeng:2012:ATP:2413176.2413205,libra}.
We use bug detection techniques to guide delta debugging's minimization
process.

It is also possible to automatically infer
performance anomalies by building probabilistic models from
collections of traces~\cite{barham2004using,Chen02pinpoint:problem}.
Our goal is to produce exact minimal causal sequences without
depending on probabilistic models, and is primarily focused on correctness
instead of performance.

Model checkers~\cite{killian2007life,nice} seek to
proactively find bugs by enumerating all possible code paths.
Model checking works well for small
programs and a small number of machines, but suffers from exponential
state explosion when run on large systems. For example, NICE~\cite{nice} took 30 hours to
model check a network with two switches, two hosts, the NOX MAC-learning
control program (98 LoC), and five concurrent
messages between the hosts. Rather than exploring all
possibilities, we discover bugs through testing.
\colin{TODO: discuss how finding the critical transition is related to finding the MCS}

\noindent{\bf Replay.} Crucial diagnostic information is often missing from traces.
Record and replay techniques~\cite{Geels:2006:RDD:1267359.1267386,lin2009towards}
instead allow users to step through executions and interactively examine the
state of the system. % in exchange for a loss in runtime performance.
Replay analysis can also be conducted offline %, without incurring runtime performance overhead,
to find code paths that lead up to software
crashes~\cite{Yuan:2010:SED:1736020.1736038} or thread schedules that reproduce
race conditions~\cite{Zamfir:2010:EST:1755913.1755946}.
Within the SDN problem space, OFRewind~\cite{ofrewind} provides
record and replay of OpenFlow channels between controllers and switches.
Manually examining long system executions can be tedious; our goal is to
automatically minimize system executions.

%Rx~\cite{qin2005rx} is a technique for improving availability: upon
%encountering a crash, it starts from a previous checkpoint, fuzzes
%the environment (\eg~random number generator seeds) to avoid triggering the same bug,
%and restarts the program. Our
%approach perturbs the inputs rather than the environment
%prior to a failure.

\noindent{\bf Root Cause Analysis.} Without perfect instrumentation,
it is often not possible to know exactly what events are occurring in a
distributed system. Root cause analysis~\cite{yemini1996,Kandula:2009:DDE:1592568.1592597}
techniques seek to identify the minimum set of failed
components (\eg~link failures) needed to explain a collection of alarms. Rather than
focusing on individual events (component failures), we seek to minimize inputs that affect the behavior
of the overall distributed system.

It is worth mentioning another goal outside the purview of distributed systems, but
closely in line with ours: program slicing~\cite{weiser1981program} is a
technique for finding the
minimal subset of a program that could possibly affect (either through control flow or data
flow) the result of a particular line of code.
Our technique can be viewed as a form of program slicing,
except that our `program' is a distributed event schedule,
and the result we are interested in is the network misconfiguration at
the end of the execution.

\eat{ % Most recent version 
\colin{Should cite Interactive Debugging for ISPs~\cite{lin2009towards}.}

Our work spans three fields: software engineering, programming languages, and
systems and networking.

\noindent{\bf Software Engineering \& Programming Languages}
Sherlog~\cite{Yuan:2010:SED:1736020.1736038} takes on-site logs from a
single program that ended in a failure as input, and applies static analysis to infer the
program execution (both code paths and data values) that lead up to the failure.
Along a similar vein, execution
synthesis~\cite{Zamfir:2010:EST:1755913.1755946} takes a program and a bug
report as input, and employs symbolic execution to find a thread schedule that will
reproduce the failure. The authors of delta debugging
applied their technique to multi-threaded (single-core) programs
to identify the minimum set of thread
switches from a thread schedule (a single input file) that reproduces
a race condition~\cite{choi2002isolating}. Chronus presents a simpler search
algorithm than delta debugging that is specific to configuration
debugging~\cite{whitaker2004configuration}.
All of these techniques focus on troubleshooting single, non-distributed
systems.

Rx~\cite{qin2005rx} is a technique for improving availability: upon
encountering a crash, it starts from a previous checkpoint, fuzzes
the environment (\eg~random number generator seeds) to avoid triggering the same bug,
and restarts the program. Our
approach perturbs the inputs rather than the environment
prior to a failure.

Model checkers such as Mace~\cite{Killian:2007:MLS:1250734.1250755} and
NICE~\cite{nice} enumerate all possible code paths taken by control software (NOX)
and identify concrete inputs that cause
the system to enter invalid configurations. Model checking works well for small
control programs and a small number of machines, but suffers from exponential
state explosion when run on large systems. For example, NICE took 30 hours to
model check a network with two switches, two hosts, the MAC-learning
control program (98 LoC), and five concurrent
messages between the hosts~\cite{nice}. Rather than exploring all
possibilities, we discover bugs through testing and systematically
enumerate subsequences of their event traces
in polynomial time.

\noindent{\bf Systems and Networking}
We share the common goal of improving troubleshooting
of software-defined networks with OFRewind~\cite{ofrewind} and
recent project ndb~\cite{handigol2012debugger}. OFRewind provides
record and replay of OpenFlow control channels, and
allows humans to manually step through and filter input traces.
We focus on testing corner cases and automatically
isolating minimal input traces.

ndb provides a
trace view into the OpenFlow forwarding tables
encountered by historical and current packets in the network.
This approach is well suited for troubleshooting hardware problems, where the
network configuration is correct but the forwarding behavior is not.
In contrast, we focus on bugs in control software; our technique
automatically identifies the control plane decisions that installed
erroneous routing entries.

Neither ndb nor OFRewind address the problem of diagnostic information
overload: with millions of packets on the wire, it can
be challenging to pick just the right subset to interactively debug.
To the
best of our knowledge, \simulator~is the first system that programmatically provides
information about precisely what caused the network to enter an invalid
configuration in the first place.

\colin{
Runtime invariant checking.
WiDS checker introduced the notion of recording
production executions to be later replayed and verified in a controlled simulation~\cite{Liu07widschecker}.
D3S made this invariant checking process realtime.
}

Trace analysis frameworks such as Pip~\cite{pip} allow developers
to programmatically check whether their expectations about the structure of
recorded causal
traces hold. MagPie~\cite{barham2004using} automatically identifies anomalous
traces, as well as unlikely transitions within anomalous traces by constructing
a probabilistic state machine from a large collection of traces and
identifying low probability paths.
Our approach identifies the exact minimal causal set of inputs without
depending on probabilistic models.

\colin{Cut:}
Network simulators such as
Mininet~\cite{handigol2012reproducible}, ns-3~\cite{ns3}, and ModelNet~\cite{Vahdat:2002:SAL:844128.844154}
are used to prototype and test network software.
Our focus on comparing diverged histories requires us
to provide precise replay of event sequences, which is in tension with the performance
fidelity goals of pre-existing simulators.

% Could remove dependency inference citation if strapped for space
Root cause analysis~\cite{yemini1996} and dependency inference~\cite{Kandula:2009:DDE:1592568.1592597}
techniques seek to identify the minimum set of failed
components (\eg~link failures) needed to explain a collection of alarms. Rather than
focusing on individual component failures, we seek to minimize inputs that affect the behavior
of the overall distributed system.

% Debug Determinism~\cite{zamfir2011debug}: suggests that replay debuggers should not seek to achieve
% perfect fidelity -- the utility of the replay can still be high, even if all failure modes can't be reproduced.
% Our approach follows this argument: can't catch everything, but it's still useful!
} % \eat most recent version

\colin{Reviewer OD: read Knowledge Plane For The
Internet~\cite{Clark:2003:KPI:863955.863957}}
