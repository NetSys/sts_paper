Our goal is to transform
event traces (`test cases') into minimal sequences of inputs.
In order to achieve this goal we are as far we know the first to
extend delta debugging~\cite{Zeller:1999:YMP:318773.318946,Zeller:2002:SIF:506201.506206}
to a distributed system, where there is an ongoing sequence of inputs
involving multiple actors rather than a single flat file fed to a single point of
execution. Delta debugging has been applied to minimize thread context switches
leading up to race conditions~\cite{choi2002isolating}, and to
prune irrelevant threads from execution traces~\cite{huang2012lean}, but this still
involves injecting a single input (the thread schedule) to a single entity (the
scheduler).

Other than delta debugging, the closest work to ours involves reasoning about
control- and data-flow dependencies
(which may be recorded at runtime~\cite{Lee:2011:TGR:1993498.1993528}, or
may be inferred by replaying once with code tracing enabled~\cite{tallam2007enabling})
in order to reduce the length of deterministic replay executions. These
techniques are tied to a single language because of the extensive effort
required for instrumentation and annotation; we choose to solve a harder
problem by not considering the internal decision processes of control
software in order to remain language independent. % Our Peek() algorithm is a
% contribution in how to infer dependencies without access to software internals
Moreover, our use of
functional equivalence relations within our domain allows us to minimize
inputs more aggressively.

We characterize the other approaches taken by the troubleshooting literature
as (i) instrumentation (tracing),
(ii) bug detection (invariant checking),
(iii) replay (of buggy executions), and
(iv) root cause analysis (of network device failures).

\vspace{0.05in}
\noindent{\bf Instrumentation.} Unstructured
log files collected at each node are the most common form of diagnostic information. The goal of
tracing
frameworks~\cite{pip,fonseca2007x,Chen02pinpoint:problem,ndb14,barham2004using}
is to produce structured logs that can be easily analyzed, such as DAGs tracking the events triggered by
requests passing through the distributed system. An example within the SDN
problem space is ndb~\cite{ndb14}, which
allows users to retroactively examine the paths dataplane packets
take through OpenFlow networks. Our goal is to filter the signal from the
noise from such diagnostic information to find the minimum needed
to understand what triggered a given bug.

\vspace{0.05in}
\noindent{\bf Bug Detection.} With instrumentation available, it becomes possible
to check expectations about the
system's state (either offline~\cite{Liu07widschecker} or online~\cite{d3s,dao2009live}), or about the paths requests take through
the system~\cite{pip}. Within the field of networking, this research is
primarily focused on verifying routing tables~\cite{hsa,hsa_realtime,anteater,khurshid2012veriflow}
or forwarding behavior~\cite{Zeng:2012:ATP:2413176.2413205,libra}.
We use bug detection techniques (invariant checking) to guide delta debugging's minimization
process.

It is also possible to automatically infer
performance anomalies by building probabilistic models from
collections of traces~\cite{barham2004using,Chen02pinpoint:problem}.
Our goal is to produce exact minimal causal sequences without
depending on probabilistic models, and we are primarily focused on correctness
instead of performance.

Model checkers~\cite{killian2007life,nice} seek to
proactively find bugs by enumerating all possible code paths.
After identifying a bug with model checking, finding a minimal code path leading to it is
straightforward. However, model checking suffers from exponential
state explosion when run on large systems. For example, NICE~\cite{nice} took 30 hours to
model check a network with two switches, two hosts, the NOX MAC-learning
control program (98 LoC), and five concurrent
messages between the hosts. Rather than exploring all
possibilities, we discover bugs through testing of selected scenarios,
which we believe fits more naturally into software engineers' chosen
way of developing and testing distributed systems.
\colin{TODO: discuss how finding the critical transition is related to finding the MCS}

\vspace{0.05in}
\noindent{\bf Replay.} Crucial diagnostic information is often missing from traces.
Record and replay techniques~\cite{Geels:2006:RDD:1267359.1267386,lin2009towards}
instead allow users to step through executions and interactively examine the
state of the system in exchange for a loss in runtime performance.
Static analysis and symbolic execution can also be applied post-hoc %, without incurring runtime performance overhead,
to find code paths that lead up to software
crashes~\cite{Yuan:2010:SED:1736020.1736038} or thread schedules that reproduce
race conditions~\cite{Zamfir:2010:EST:1755913.1755946}.
Within the SDN problem space, OFRewind~\cite{ofrewind} provides
record and replay of OpenFlow channels between controllers and switches.
Manually examining long system executions can be tedious, and our goal is to
automatically minimize such executions; that is, we minimize the length of
the execution in question so that developers find it easier to identify the
problematic code through replay or other means.

%Rx~\cite{qin2005rx} is a technique for improving availability: upon
%encountering a crash, it starts from a previous checkpoint, fuzzes
%the environment (\eg~random number generator seeds) to avoid triggering the same bug,
%and restarts the program. Our
%approach perturbs the inputs rather than the environment
%prior to a failure.

\vspace{0.05in}
\noindent{\bf Root Cause Analysis.} Without perfect instrumentation,
it is often not possible to know exactly what events are occurring (\eg~which
network components have failed) in a
distributed system. Root cause analysis~\cite{yemini1996,Kandula:2009:DDE:1592568.1592597}
seeks to reconstruct those unknown events from limited monitoring data.
Here we know exactly which events occurred, but
seek to identify the minimal set of events that could trigger a particular bug.

\colin{Cut if we need space:}
\vspace{0.05in}
It is worth mentioning another goal outside the purview of distributed systems, but
closely in line with ours: program slicing~\cite{weiser1981program} is a
technique for finding the
minimal subset of a program that could possibly affect (either through control flow or data
flow) the result of a particular line of code.
Our goal is to slice the temporal dimension of an execution rather than the
code dimension.
%Our technique can be viewed as a form of program slicing,
%except that our `program' is a distributed event schedule,
%and the result we are interested in is the network misconfiguration at
%the end of the execution.

\eat{ % Most recent version 
\colin{Should cite Interactive Debugging for ISPs~\cite{lin2009towards}.}

Our work spans three fields: software engineering, programming languages, and
systems and networking.

\noindent{\bf Software Engineering \& Programming Languages}
Sherlog~\cite{Yuan:2010:SED:1736020.1736038} takes on-site logs from a
single program that ended in a failure as input, and applies static analysis to infer the
program execution (both code paths and data values) that lead up to the failure.
Along a similar vein, execution
synthesis~\cite{Zamfir:2010:EST:1755913.1755946} takes a program and a bug
report as input, and employs symbolic execution to find a thread schedule that will
reproduce the failure. The authors of delta debugging
applied their technique to multi-threaded (single-core) programs
to identify the minimum set of thread
switches from a thread schedule (a single input file) that reproduces
a race condition~\cite{choi2002isolating}. Chronus presents a simpler search
algorithm than delta debugging that is specific to configuration
debugging~\cite{whitaker2004configuration}.
All of these techniques focus on troubleshooting single, non-distributed
systems.

Rx~\cite{qin2005rx} is a technique for improving availability: upon
encountering a crash, it starts from a previous checkpoint, fuzzes
the environment (\eg~random number generator seeds) to avoid triggering the same bug,
and restarts the program. Our
approach perturbs the inputs rather than the environment
prior to a failure.

Model checkers such as Mace~\cite{Killian:2007:MLS:1250734.1250755} and
NICE~\cite{nice} enumerate all possible code paths taken by control software (NOX)
and identify concrete inputs that cause
the system to enter invalid configurations. Model checking works well for small
control programs and a small number of machines, but suffers from exponential
state explosion when run on large systems. For example, NICE took 30 hours to
model check a network with two switches, two hosts, the MAC-learning
control program (98 LoC), and five concurrent
messages between the hosts~\cite{nice}. Rather than exploring all
possibilities, we discover bugs through testing and systematically
enumerate subsequences of their event traces
in polynomial time.

\noindent{\bf Systems and Networking}
We share the common goal of improving troubleshooting
of software-defined networks with OFRewind~\cite{ofrewind} and
recent project ndb~\cite{handigol2012debugger}. OFRewind provides
record and replay of OpenFlow control channels, and
allows humans to manually step through and filter input traces.
We focus on testing corner cases and automatically
isolating minimal input traces.

ndb provides a
trace view into the OpenFlow forwarding tables
encountered by historical and current packets in the network.
This approach is well suited for troubleshooting hardware problems, where the
network configuration is correct but the forwarding behavior is not.
In contrast, we focus on bugs in control software; our technique
automatically identifies the control plane decisions that installed
erroneous routing entries.

Neither ndb nor OFRewind address the problem of diagnostic information
overload: with millions of packets on the wire, it can
be challenging to pick just the right subset to interactively debug.
To the
best of our knowledge, \simulator~is the first system that programmatically provides
information about precisely what caused the network to enter an invalid
configuration in the first place.

\colin{
Runtime invariant checking.
WiDS checker introduced the notion of recording
production executions to be later replayed and verified in a controlled simulation~\cite{Liu07widschecker}.
D3S made this invariant checking process realtime.
}

Trace analysis frameworks such as Pip~\cite{pip} allow developers
to programmatically check whether their expectations about the structure of
recorded causal
traces hold. MagPie~\cite{barham2004using} automatically identifies anomalous
traces, as well as unlikely transitions within anomalous traces by constructing
a probabilistic state machine from a large collection of traces and
identifying low probability paths.
Our approach identifies the exact minimal causal set of inputs without
depending on probabilistic models.

\colin{Cut:}
Network simulators such as
Mininet~\cite{handigol2012reproducible}, ns-3~\cite{ns3}, and ModelNet~\cite{Vahdat:2002:SAL:844128.844154}
are used to prototype and test network software.
Our focus on comparing diverged histories requires us
to provide precise replay of event sequences, which is in tension with the performance
fidelity goals of pre-existing simulators.

% Could remove dependency inference citation if strapped for space
Root cause analysis~\cite{yemini1996} and dependency inference~\cite{Kandula:2009:DDE:1592568.1592597}
techniques seek to identify the minimum set of failed
components (\eg~link failures) needed to explain a collection of alarms. Rather than
focusing on individual component failures, we seek to minimize inputs that affect the behavior
of the overall distributed system.

% Debug Determinism~\cite{zamfir2011debug}: suggests that replay debuggers should not seek to achieve
% perfect fidelity -- the utility of the replay can still be high, even if all failure modes can't be reproduced.
% Our approach follows this argument: can't catch everything, but it's still useful!
} % \eat most recent version

\colin{Reviewer OD: read Knowledge Plane For The
Internet~\cite{Clark:2003:KPI:863955.863957}}
