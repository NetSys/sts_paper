The core piece of infrastructure we need to realize our approach is a
mechanism to produce and replay execution logs. Building such a system comes
with many challenges; unlike the example applications described
by the delta debugging paper~\cite{Zeller:1999:YMP:318773.318946}, the system we are troubleshooting is not a
single program--it encompasses all the nodes and links of a distributed system,
including controllers, switches, and end-hosts, where asynchrony
makes it difficult to reliably replay inputs.

Both of the SDN controller vendors we are in contact with~\cite{nicirahomepage,bigswitch} employ a team of QA
engineers to fuzz test their controller software on small to large testbeds of switches and hosts.
As depicted in Figure~\ref{fig:qa_cluster}, this fuzz testing infrastructure
consists of the controller software under test, the network testbed (which may
be software or hardware), and a centralized
test orchestrator
that chooses random input sequences, drives the behavior of the testbed,
periodically checks invariants, and manages log files. When a bug is discovered, it is first triaged
by a human, then logs are collected and sent to a developer for further troubleshooting.

\begin{figure}[t]
    %\hspace{-10pt}
    \includegraphics[width=3.25in]{../diagrams/architecture/qa_cluster.jpg}
    \caption[]{\label{fig:qa_cluster} Typical QA testbed. A centralized test
    orchestrator drives the behavior of the network, and checks for bugs in
    the controllers.}
\end{figure}

Engineering organizations with existing
QA testbeds can add delta debugging to their test
orchestrator, and optionally insert interposition points throughout the
testbed to control event ordering during replay.
In this way they can continue simulating large scale networks with
the switches, middleboxes, hosts, and routing protocols they had already
chosen to include in their QA testbed.

We do not have access to such a QA testbed however, and instead built our own
integration testing framework to discover bugs and
perform replay. Our framework mocks out the control plane
behavior of network devices in lightweight software switches and hosts (with
support for minimal data plane forwarding)
within a single-threaded process.\footnote{To achieve a deterministic order of
events during replay either we had to serialize events across threads via
mutual exclusion or replay them in a single thread, and it turned out that
the latter was cleaner and does not substantially impact performance.} We then
run the control software on
top of this mock network and connect the software switches to the controllers as if they were true
network devices, such that the controllers believe they are configuring a true
network. The mock network interposes and buffers messages on all communication
channels, allowing it to delay, drop, or reorder
messages as needed to induce failure modes during testing. This interposition
allows us to replicate many of the failure modes caused by asynchrony in real
networks, in a more deterministic manner. During
replay, we use these buffers to enforce event orderings by
matching messages in the buffers by their fingerprints and managing
the order message are let through. The overall simulation architecture is depicted in
Figure~\ref{fig:architecture}.

\begin{figure}[t]
    %\hspace{-10pt}
    \includegraphics[width=3.25in]{../diagrams/architecture/Debugger_Architecture.pdf}
    \caption[]{\label{fig:architecture} \projectname~runs mock
    network devices, and interposes on all communication
    channels. \colin{Bad figure}}
\end{figure}

\begin{table}
\centering
\begin{tabular}{|l|l|}
\hline
Input Type &  Implementation \\
\hline
\hline
Switch failure/recovery & TCP teardown \\
\hline
Controller failure/recovery & SIGKILL \\
\hline
Link failure/recovery & ofp\_port\_status \\
\hline
Controller partition & iptables \\
\hline
Dataplane packet injection & network namespaces \\
\hline
Dataplane packet drop & data plane interposition \\
\hline
Dataplane packet delay & data plane interposition \\
\hline
Host migration & ofp\_port\_status \\
\hline
Control message delay & control plane interposition \\
\hline
Non-deterministic TCAMs & modified switches \\
\hline
\end{tabular}
\caption{Input types currently supported by \projectname}
\label{tab:inputs}
\end{table}

\begin{table}
\centering
\begin{tabular}{|l|l|}
\hline
All-to-all reachability & Loop freeness \\
\hline
Blackhole freeness & Controller liveness \\
\hline
POX ACL compliance & ONOS flow routing compliance \\
\hline
\end{tabular}
\caption{Invariant checks currently supported by \projectname}
\label{tab:invariants}
\end{table}

We begin by using our mock network to perform testing on controllers to find
bugs. Our most common use case involves generating randomly chosen input
sequences~\cite{Miller:1990:ESR:96267.96279}, feeding them to controller(s),
and monitoring
invariants at chosen intervals.\footnote{The full list of invariants currently
supported is shown in Table~\ref{tab:invariants}.}
We also run the mock network interactively
so that we can examine the state of any part of the network,
observe and manipulate messages, and follow our
intuition to induce orderings that we believe may trigger bugs.
Either way, controlling the inputs from a single location
allows our test framework to record a global, serial
event ordering.

After discovering an invariant violation of interest, our system replays
the logged sequence of inputs (shown in Table~\ref{tab:inputs}). For example,
the system replays link failures
by disconnecting the edge in the mock network, and sending a
ofp\_port\_status~\cite{openflow} message from the adjacent switches to their parent controller(s).

\projectname~(\projectmeaning) is our realization of this system.
\projectname~is implemented in roughly 18,000 lines of Python in
addition to the Hassel network invariant checking library~\cite{hsa}.
\projectname~also optionally makes use of Open vSwitch~\cite{pfaff2009extending} as an interposition point for
messages sent between distributed controllers. We have
made the code
for \projectname~publicly available (anonymized), % at \href{http://ucb-sts.github.com/sts}{ucb-sts.github.com/sts},
and have discussed the logistics of deploying it with several SDN companies.

\colin{TODO: Discuss the value of replay for root causing here? Or just let
the use cases speak for themselves?}

%How do we ensure the same scheduling order of controller processes? (see
%Section 4.5 of ReVirt\cite{Dunlap:2002:REI:844128.844148}). We observe that the controllers do not use any shared
%memory communication.}

\subsection{Coping with Non-Determinism}

Non-determinism in the execution of concurrent processes stems from
differences in system call return values, process scheduling decisions (which can
even affect the result of individual instructions, such as x86's
interruptable block memory instructions~\cite{Dunlap:2002:REI:844128.844148}),
and asynchronous signal
delivery. These sources of non-determinism can affect whether \projectname~is
able to reproduce the original bug during replay.

Most testing systems, such as the QA testing frameworks we are
trying to improve, do not attempt to mitigate non-determinism; they assume
that developers will be able replicate bugs by attempting to reproduce the
same conditions as the original test run, else track down the the root cause
only by
inspecting the raw execution logs.

\projectname's main approach to coping with non-determinism
is to replay each subsequence chosen
by delta debugging multiple times. If observing the bug
is an independent and identically distributed random variable (an optimistic
but not completely unreasonable assumption) with some
underlying probability $p$, then $n$
replays will observe the bug with probability $1-(1-p)^{n}$. This exponential
works strongly in our favor; for example, even if the original bug is
triggered in only 20\% of replays, the probability that we will not trigger
it during an intermediate replay is approximately
1\% if we replay 20 times per subsequence.
\colin{Mention optimizing for reproducibility?}

\subsection{Mitigating Non-Determinism}
\label{subsec:mitigating}

In cases where non-determinism's effect on replayability is substantial,
one might also seek to mitigate or prevent non-determinism altogether.
Deterministic replay techniques~\cite{Dunlap:2002:REI:844128.844148,Geels:2006:RDD:1267359.1267386},
which precisely record and replay system call return values and process
scheduling decisions,
may be deployed to reliably produce bugs in exchange for a loss in performance.
Unfortunately these techniques do not allow the user to make any modifications to the inputs fed to the
processes during replay, since changes may cause the remaining execution to
subtly differ (\eg~the sequence numbers of packets may all differ).
This precludes the possibility of employing (fully) deterministic replay techniques in
\projectname, since our goal is precisely to modify the inputs fed to the
distributed system.
%And even if these sources were
%eliminated, it would not be possible to achieve perfectly deterministic
%replay in all cases without full visibility into internal events--a daunting
%instrumentation task.

We therefore designed \projectname~to be as resilient to non-determinism as is
practically feasible, while avoiding modifications to control software whenever possible.
We began by designing \projectname~to be in a position to
record and replay all network events in serial order. We also ensured that all
datastructures within \projectname~were not effected by randomness; for example,
we avoid using hashmaps that hash keys according to their memory address,
and sort all list return values.

We also optionally interpose on or modify the controller software itself.
Routing the {\tt gettimeofday()} syscall through \projectname~helps ensure that timers fire
at the appropriate point.\footnote{When the pruned trace differs from the original, we make a
best-effort guess at what the return values of these calls should be. For example,
if the altered execution invokes {\tt gettimeofday()} more times than we recorded
in the initial run, we interpolate the timestamps of neighboring events.}\footnote{Only supported for POX and Floodlight at the moment}
When sending data over multiple sockets, the operating system exhibits
non-determinism in the order it schedules the I/O operations.
\projectname~optionally ensures a deterministic order of messages
by multiplexing all sockets
onto a single true socket. On the controller side \projectname~currently
overrides socket functionality by modifying the controller control
software itself,\footnote{Only supported for POX at the moment.} although this
could also be achieved with a libc shim layer~\cite{Geels:2006:RDD:1267359.1267386}.
%In the future we plan to implement deterministic message ordering without code modifications by
%loading a shim layer on top of
%libc (similar to liblog~\cite{Geels:2006:RDD:1267359.1267386}).

\projectname~may need visibility into the control software's internal state
transitions to properly maintain happens-before relations during replay. We
gain visibility by making a
small change to the control software's logging library\footnote{Only supported
for POX and Floodlight at the moment.}: whenever a control process executes a log
statement, which typically indicate that an important state transition is about to take
place, we notify \projectname. Such coarse grained visibility into internal
state transitions does not handle all cases, but we find in practice that is often
sufficient.\footnote{We discuss this limitation further in \S\ref{subsec:non_goals}.} Beyond
providing visibility, we can also optionally use
logging interposition as a
synchronization barrier, by blocking the process when it executes crucial logging statements
until \projectname~explicitly tells the process that it may proceed.

%If blocking was enabled
%during recording, we force the control software to block at internal state
%transition points again during replay
%until \projectname~gives explicit acknowledgment.

%\subsection{Timing Heuristics}
%\label{subsec:timing_heuristics}
%\colin{Not sure if this is the right place for this}
%\colin{TODO: add more experiments here}
%
%Basically trial and error. Use visualization tools (described in \S\ref{subsec:root_causing}) to validate
%the effectiveness of these techniques by comparing replay executions with and
%without them enabled.
%
%\noindent{\bf Scheduling Events}
%Include discussion of dumb event scheduler: only maintains deltas between
%events. Our current event scheduler on the other hand accounts for time taken
%to run the event, i.e. delta from real run - time.time() - time we last
%injected. Especially important for timeouts! Not just injection time.
%BTW, we have found cases where one worked and the other didn't. Our current
%one is mostly better though.
%
%\noindent{\bf peek()}
%We had this crazy algorithm that sorta worked. See algorithm.tex.
%
%\noindent{\bf Whitelisting control plane packets}
%We whitelist some packets LLDP. (problems therein.)
%We also tried another algorithm: let new
%internal messages through, keep a window of what we expect. If we don't
%expect, let it through. Problem: false positive: accidentally let through
%internal messages too early, then we time out on them after.
%
%\noindent{\bf Using logging statements as barriers}
%Need to be careful: if controller goes through a new internal state
%transition, it
%will block. STS therefore needs to track wether the controller becomes blocked
%unexpectedly, and unblock it to proceed with the rest of replay.
%
%\noindent{\bf Predicting whether a dataplane packet is unexpected}
%In general, timeouts during replay strongly effect the execution. Timeouts are
%much larger than the execution time of an event. This is ameliorated somewhat
%by buffers, but the timing with the controller may be off somewhat.
%Dataplane permit events constitue a substantial portion of events. In an
%attempt to reduce timeouts, we tried to make dataplane permit the default,
%such that we never time out on them. To do this we keep a window of expected
%dp *drops* into the future. We then allow the dp packet through, *unless* we
%see that we should drop it in the near future.

\subsection{Root Causing Tools}
\label{subsec:root_causing}

Throughout our experimentation with \projectname, we often found that
minimized event traces alone did not provide us with enough information to
pinpoint the root causes of bugs. We therefore implemented a number of
complementary root
causing tools within \projectname,
which we use along with Unix utilities to help us complete the final
stage of debugging. We illustrate in \S\ref{sec:evaluation}~how exactly we use
these tools.

\noindent{\bf OFRewind}. \projectname~supports an interactive replay mode
similar to OFRewind~\cite{ofrewind} that allows troubleshooters to query the
state of the network throughout replay, filter subsets of the events, check
additional invariants, and
even induce new events that were not part of the original event trace.
Similar to OFRewind, we do not run concurrent controller processes while the
user is interactively performing replay, since proper replay across
concurrent processes requires precise timing.
Instead, \projectname~replays the exact OpenFlow commands from the
original trace to the switches, and creates mock TCP connections that drop
whatever messages the switches attempt to send back to the controllers.

\noindent{\bf Packet Tracing}. Especially for SDN controllers that react to
flow events, we found it useful to trace the path of individual
packets throughout the network. \projectname~includes tracing instrumentation
similar
to XTrace~\cite{fonseca2007x} and ndb~\cite{ndb14} for this
purpose.

\noindent{\bf OpenFlow Reduction}. The OpenFlow commands sent by controller software
are often somewhat redundant. For example, controllers may override routing
entries, allow them to expire, or periodically flush the
contents of flow tables and later repopulate them. \projectname~includes a
tool for filtering out such redundant messages,
leaving only those commands that are directly relevant for invalid network
configurations such as loops or blackholes.

\noindent{\bf Event Visualization}. Understanding the timing of messages and internal
state transitions is a crucial part of troubleshooting distributed systems.
STS includes two visualization tools designed to aid with this task. First, we
include a tool to visualize space-time diagrams~\cite{Lamport:1978:TCO:359545.359563}
of event traces, such as the one depicted in Figure~\ref{fig:example}.
Second, we include a tool to visually highlight event ordering differences
between two or more event traces, which is especially useful for comparing the behavior of
intermediate delta debugging replays when the original trace exhibits a high degree of non-determinism.

\subsection{Scaling and Parallelization}

When minimizing very large event traces we found that the garbage collector
for our mock network often became overwhelmed (causing the process to slow down
substantially) after replaying several subsequences, since each replay could
occupy gigabytes of memory space with many small objects.
After observing this behavior we modified \projectname~to fork a process
(either local or remote) for each subsequence chosen by delta debugging,
and gather the results of the replay via RPC. This alleviates the slow down,
since the memory footprint of the replay process is cleaned up all at once by the
operating system once the
replay terminates, and the
main delta debugging process keeps little memory itself.
As an added benefit, this architectural change allows us to support
parallelized delta debugging across multiple cores or machines.

\subsection{Enabling Analysis of Production Logs}
\label{subsec:production_logs}
\colin{Cut if we need space.}

\projectname~does not currently support minimization of production (as opposed
to QA) logs.
Here we present a sketch of how \projectname~might support production logs as input.

While \simulator~takes as input a single, totally-ordered log of the events in the
distributed system, production systems maintain a log at each node.
Production systems would need to include Lamport
clocks on each message~\cite{Lamport:1978:TCO:359545.359563} or have
sufficiently accurate clock
synchronization~\cite{corbett2012spanner} to obtain a partial global ordering
consistent with the happens-before relation.\footnote{
Note that a total ordering is not needed, since it is permissible
for \simulator~to reorder concurrent events from
the production run so long as the happens-before relation is
maintained~\cite{Fischer:1985:IDC:3149.214121}.} Inputs would also
need to need to be logged in sufficient detail for \projectname~to
replay a synthetic version of the input that is indistinguishable (in terms
of control plane messages) from the original.

Without care, a single input event may appear multiple times in the
distributed logs. A failure of the master node, for example, could be independently
detected and logged by all other replicas. The most robust way to
avoid redundant input events would be to employ perfect failure
detectors~\cite{chandra1996unreliable}, which log a failure iff
the failure actually occurred.\footnote{Perfect failure detectors can be
implemented in partially synchronous distributed systems by explicitly killing
nodes that are suspected to be down.} % Ensuring that a single failure detector is in charge of logging node failure
% events guarantees that redundant events do not appear.
Alternatively, one
could employ root cause analysis
algorithms~\cite{yemini1996} or manual inspection to consolidate redundant
alarms.

\colin{Cut if we add a section on snapshots}
Finally, some care would be needed to prevent the logs from growing so large that
\simulator's runtime becomes intractable. Here, causally consistent
snapshots~\cite{Chandy:1985:DSD:214451.214456} would allow \projectname~to
bootstrap its simulation from the last snapshot before the failure rather than
replaying from the beginning of the log.
%If the MCS starting from this snapshot is empty, it could iteratively move backwards, starting from earlier
%snapshots.

\subsection{Limitations}
\label{subsec:non_goals}

Having detailed the specifics of our approach we now
clarify the scope of our technique's use.

\noindent{\bf Partial Visibility.} Our event scheduling algorithm assumes that
it has visibility into the occurrence of all relevant internal events. For
some controllers this may involve substantial instrumentation effort beyond
pre-existing log statements.

\noindent{\bf Non-determinism.} Non-determinism
is fundamental in networks. When non-determinism is present
\projectname~(i) replays multiple times per subsequence, and (ii) employs
software techniques for mitigating non-determinism, but it may nonetheless
output a non-minimal causal sequence. In the common case \projectname~is
better than what developers had before, since developers generally
do not have tools for reproducing non-deterministic bugs.
In the worst case \projectname~leaves the
developer where they started: an unpruned log.

%In particular our technique is not designed to reproduce bugs
%involving non-determinism within a single controller (\eg~race-conditions between threads);
%we focus on coarser granularity errors (\eg~incorrect failover logic).

\eat{
\noindent{\bf Troubleshooting vs.\ Debugging.} Our technique is a troubleshooting tool, not a debugger;
by this we mean that our approach helps identify and localize inputs that
trigger erroneous behavior, but it does not directly identify which
line(s) of code cause the error.}

\noindent{\bf Bugs Outside the Control Software.} Our goal is not to find the root
cause of individual component failures in the system (\eg~misbehaving routers,
link failures). Instead, we focus on
how the distributed system as a whole reacts to the occurrence of such inputs.
%If there is a bug in your switch, you will need to contact your hardware vendor;
%if you have a bug in your policy specification, you will need to take a closer look at what you specified.

\noindent{\bf Globally vs.\ Locally Minimal Input Sequences.}
Our approach is not guaranteed to find the globally minimal
causal sequence from an input trace, since this involves enumerating the powerset of
$E_L$ in the worst case (a $O(2^n)$ operation).
The delta debugging algorithm we employ does provably find a
locally minimal causal sequence~\cite{Zeller:1999:YMP:318773.318946},
meaning that if any input from the sequence is pruned, no invariant violation
occurs.

\noindent{\bf Correctness vs.\ Performance.}
We are primarily focused on correctness bugs, not performance bugs.

\noindent{\bf Bugs Found Through Fuzzing.}
We generate bugs primarily through fuzz testing, not by finding them in
operational traces. There is a substantial practical hurdle in instrumenting
operational systems to produce logs that can be injected into our system, as
discussed in \S\ref{subsec:production_logs}.
% , and we have not addressed those issues yet.
%In practice some bugs
%found through fuzzing will not be considered worthwhile to investigate.

\noindent{\bf Scaling.}
Our discussions with companies with large SDN deployments suggest that scaling to the size of the
large logs they collect will be a substantial challenge.
On the other hand, the fact that these logs are so large makes the need for finding MCSes even more acute.

\eat{
\noindent{\bf Proactive vs.\ Reactive Configuration.} We focus primarily on
\emph{proactive} configuration, where controllers react to policy and topology changes, but
not necessarily individual packets or flows events in the
dataplane.\footnote{Production controllers typically adopt this model for
performance reasons.}
The main challenge in extending our approach to reactive controllers is
achieving efficient simulation of dataplane traffic.
\andi{Could cut this. We actually find reactive bugs}
}

