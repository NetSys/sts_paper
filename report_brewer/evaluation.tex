
    \begin{figure}[t]
        \hspace{-10pt}
        \includegraphics[width=3.25in]{../graphs/crummy_overhead_graph/graph.pdf}
        \caption[]{\label{fig:anteatertime} Execution time to run invariant checker over full mesh and randomly generated network topologies of 5 to 250 nodes.} 
    \end{figure}

    We are building an implementation of CLINT to debug SDN networks running the POX network operating system~\cite{pox}.
    
    POX is an implementation of the NOX network operating system built in Python.
    Currently in POX, applications are written for a single controller with only minimal virtualization.
    To study \projectname{}'s ability to capture virtualization errors, we extended POX
    to support a simple virtualization layer that presents switches as Python objects to the control application.
    The virtualization layer does not yet support more complex virtualization such as `big switch' representations 
    of the network; in future work we plan to extend it to so.
    To study bugs that arise between replicated controllers, we also extended POX to support replicated controllers
    with eventually-consistent shared state.

    The current implementation of CLINT supports three features outlined in
    \S\ref{sec:architecture}, a traffic and topology `fuzzer' for input
    generation, an invariant checker to detect bugs that manifest in the
    physical network, and a preliminary mechanism to capture consitency bugs in topologies
    with multiple control servers.
    
    \projectname{}'s physical network invariant checker is an extended deployment of Anteater~\cite{anteater}, which is extensively evaluated in the Anteater paper.

    Since we are running Anteater in a loop over a potentially large
    configuration space, it is important that the runtime of our system is not
    prohibitively long. We have evaluated the overhead of iterative invariant checks by running our 
    implementation over various network topologies with sizes scaling from
    five to 250 switches, a full mesh network (with each node connected to $n - 1$ other nodes),
    and a randomly generated topology with each node randomly connected to 5
    or 8 other nodes.
    We ran the invariant checker with rounds of 100 inputs from the input generator, with invariant checks run after every fifth input (for a total of 20 runs of the invariant checker).
    The time for the invariant checker to evaluate the network is plotted in Figure~\ref{fig:anteatertime}.
   
    As shown in the diagram, the runtime of the system for small numbers of
    switches is quite reasonable. As SAT solving is an NP-complete problem
    however, even with heuristics the runtime increases exponentially with the
    number of switches and the size of the FIBs. We note that there are two
    orthogonal solutions to decrease the runtime of our system. First, we are
    looking into methods to invoke Anteater at well-chosen intervals
    throughout the exploration of the configuration space to minimize the
    number of necessary invocations. Second, we are working with the creators
    of Anteater to develop `best-effort` invariant checks that to do not
    require a full SAT analysis.

\subsection{Next Steps}
    The previously described experiments show that our implementation of \projectname{} (1) can detect errors like forwarding loops that manifest entirely in the physical network (by using Anteater), (2) can detect physical network errors that manifest as the result of a specific sequence of flows to reach the network controller (by using \projectname{}'s network fuzzer), and (3) imposes reasonable performance requirements to analyze the SDN stack for these errors.
    To complete this analysis, we are working with engineers at Nicira Networks to obtain in-depth bug reports describing errors encountered by real SDN deployments.
    In future work, we plan to replicate these bugs in our SDN testbed and investigate whether \projectname{} can detect them.
    In addition, we are working with researchers at the International Computer Science Institute (ICSI) to obtain copies of their own SDN control applications from their research, and apply \projectname{} to their implementations to investigate whether \projectname{} can detect any new bugs.

    Longer term, as the \projectname{} implementation matures to include multi-layer correspondence checks and cross-controller consistency checks (as described in \S\ref{sec:architecture}), we will apply \projectname{} to a broader class of bugs and see what fraction of Nicira bug reports \projectname{} can capture, and what kinds of new bugs it detects in control applications from ICSI. 
