The ecosystem of troubleshooting tools for computer networks is desperately
sparse; relatively little progress has been made since in the invention of
{\tt traceroute} in 1987~\cite{traceroute}. This is in stark contrast with the software 
industry, where a 10 billion dollar market
exists for testing, debugging, and formally proving the correctness of
software products. The emergence of SDN yields the
opportunity to have a much more sophisticated set of troubleshooting and
debugging tools. Consider, for example, a recent quite from Urs
H\"oelzle~\cite{urs}:

\begin{quote}
One of the strongest benefits with SDN for Google has been the robust
testing capabilities available with SDN. H\"olzle reported that the
transition from traditional to OpenFlow networks was nearly seamless and
they encountered far fewer challenges than expected. Much of what made
this possible was the ability to very easily simulate backbone-scale
network environments virtually, including the ability to mirror
production event streams in testing environments, which allowed Google
to identify and fix all the bugs in advance.
\end{quote}

The SDN platform's $raison\text{ }d'\hat{e}tre$ is to 
hide complexity from control applications. To this end, modern platforms perform
replication, resource arbitration, failure recovery, and network 
virtualization on the control application's behalf. 

While these measures are effective at simplifying control applications,
they do not remove any complexity from the overall system. Rather,
they merely move the complexity
from control applications into the underlying SDN platform. 

As in any software system, complexity increases the probability of
bugs. SDN system developers' hope is that the platform will eventually become
stable, such that operators can safely assume correctness of lower layers.
Nevertheless, the evolution of SDN is far too nascent to provide stability
today. When network operators encounter erratic behavior in their network
they are currently forced to manually trace through
multiple layers of abstraction: policy-specification, virtualization logic,
distribution logic, and network devices. 

\colin{troubleshooting != debugging. Troubleshooting is about clarifying
symptoms. Debugging assumes a well-defined symptom, and finds the root cause.}

Besides {\it ad-hoc} measurement tools,
the predominant troubleshooting mechanism in SDN is
log analysis: manually specifying log statements at relevant points throughout the system;
collecting and aligning distributed log files; and analyzing the
results {\it post-hoc} when a error is encountered in production. Log analysis
is an important component of the troubleshooting process, but it is deficient
as a stand-alone solution: logs events
are enormous in number, impossible to aggregate into a single serial
execution of the system, and often at the wrong level of granularity.

Recent work has contributed troubleshooting techniques focused on the highest (control
application) and lowest (dataplane forwarding tables) layers of the SDN stack.
NICE applies concolic execution and model checking to SDN control
applications, thereby automating the testing process and catching bugs before
they are deployed~\cite{nice}. Anteater~\cite{anteater} and HSA~\cite{hsa}
introduce mechanisms for checking static invariants in the dataplane.
Nonetheless, no automated troubleshooting mechanism exists for problems that span
multiple layers of the SDN stack, specifically those caused by bugs in the platform
itself.

%The utility of new programming models is heavily dependent on
%the usability of their troubleshooting mechanisms.
%Similarly, we think it highly undesirable to deploy SDN-based
%networking without a viable troubleshooting paradigm. 

In this paper we present \projectname{}, a framework for troubleshooting
network policy violations caused by bugs in the
SDN platform. \projectname{} localizes the root cause
of policy-violations along two dimensions, which we describe in turn:
component, and time.

\noindent{\bf Correspondence Checking}. We observe that the structure of the
SDN platform (graphs at every layer) enables a straightforward algorithm for
checking that control applications' policies are implemented correctly in
the physical network. Our algorithm, `correspondence checking',
enumerates all policy-violations present in the network at a given point in
time, providing a crisp determination of the scope of a policy-violation in the
network as well as the range of inputs that produce it. Moreover,
by checking correspondence between intermediate layers of the
SDN platform, \projectname{} is able to localize the particular component responsible 
for a given policy-violation.

% HSA helps us get a crisp definition of the policy-violation (scope of
% inconsistency in the network, range of inputs that produce it

\noindent{\bf \Simulator{}.} Like any distributed
system, transient policy-violations due to failures and delays are 
common, especially at large scale. We present \simulator{}, a semi-automated
technique to differentiate pernicious and benign policy-violations.
By iteratively replaying the execution of
the system and pruning out events that are not causally related to pernicious
policy-violations, \simulator{} localizes the minimal causal set of network events
leading up to a policy-violation. 

In evaluating \projectname{} on
three controller platforms -- Frenetic, Floodlight, and POX -- we find N bugs,
including isolation breaches,
faulty failover logic, and consistency problems between replicated
controllers. We further demonstrate the feasibility of deploying
correspondence checking and \simulator{} on production networks,
finding that correspondence checking can enumerate all policy-violations in a
fat-tree network of 100,000 hosts in under N seconds.

The rest of this paper is organized as follows. In \S\ref{sec:overview},
we present an overview of the SDN stack and its failure modes.
In \S\ref{sec:approach} we present correspondence checking and
\simulator{} in detail. In \S\ref{sec:evaluation} we present
a use-case and a preliminary performance evaluation.
Finally, in \S\ref{sec:related_work} we discuss related work,
and in \S\ref{sec:conclusion} we conclude.
