Our work spans three fields: software engineering, systems and networking, and
programming languages.

\noindent{\bf Software Engineering}
The software engineering community has developed a long line of tools for automating
aspects of the troubleshooting process.

Sherlog~\cite{Yuan:2010:SED:1736020.1736038} takes as input production logs from a
program that ended in a failure, and applies static analysis to identify the minimum subset of the
program execution (both code paths and data values) that lead up to the failure.
Along a similar vein, execution
synthesis~\cite{Zamfir:2010:EST:1755913.1755946} takes a program and a bug
report as input, and employs symbolic execution to find a thread schedule that will
reproduce the failure. Delta
debugging~\cite{Zeller:2002:SIF:506201.506206} as well seeks to find a minimal
subset of an input (always a single file) that leads to a failure. As one
example, delta debugging has been applied to multi-threaded (single-core) programs
to identify the minimum set of thread
switches from a thread schedule (a single file) that reproduces
a race condition~\cite{choi2002isolating}. Chronus presents a simpler search
algorithm than delta debugging that is specific to configuration
debugging~\cite{whitaker2004configuration}.
All of these techniques focus on troubleshooting single, non-distributed programs.

Rx~\cite{qin2005rx} is a technique for improving availability: upon
encountering a crash, it starts from a previous checkpoint, fuzzes
the environment (\eg~random number generator seeds) to avoid triggering the same bug,
and restarts the program. While Rx is not directly relevant to troubleshooting,
our work can be viewed as perturbing the inputs rather than the environment
prior to a failure.

\noindent{\bf Systems and Networking}
The systems and networking community has also developed a substantial
literature on tools for testing and troubleshooting.

Network simulators and emulators such as
Mininet~\cite{handigol2012reproducible} and ns-3~\cite{ns3}
are used to prototype and test control software. Most of these frameworks are focused
primarily on performance fidelity. In contrast, \projectname~is a simulator focused on
precise event orderings and replayable executions, which is significantly harder to achieve with
emulation.

Replay techniques such as OFRewind~\cite{ofrewind} allow developers to interactively
trace and prune the inputs that lead up to errant behavior. Tracing tools provide a view
requests' paths through the dataplane~\cite{handigol2012debugger} or control
plane~\cite{fonseca2007x}. Frameworks for tracing such as Pip~\cite{pip} extend this
concept and instrument general-purpose distributed systems
with code to record, display, and check invariants on causal paths throughout
live executions. All of these techniques involve some amount of human
intervention; we focus on automating the diagnosis process.

Root cause analysis~\cite{yemini1996} and dependency inference~\cite{Kandula:2009:DDE:1592568.1592597}
techniques seek to identify the minimum set of failed
components (\eg~link failures) needed to explain a collection of alarms. Rather than
focusing on individual component failures, we seek to minimize inputs that affect the behavior
of the overall distributed system.

% Debug Determinism~\cite{zamfir2011debug}: suggests that replay debuggers should not seek to achieve
% perfect fidelity -- the utility of the replay can still be high, even if all failure modes can't be reproduced.
% Our approach follows this argument: can't catch everything, but it's still useful!

\noindent{\bf Programming Languages}
Finally, the programming languages community has developed
a wide range of verification and static analysis techniques.

% NICE isn't *really* from the PL community
Model checkers such as NICE~\cite{nice} enumerate all possible code paths taken by control software (NOX)
and identify concrete inputs that cause
the network to enter invalid configurations. NICE works well for small
control programs and a small number of machines, but suffers from exponential
state explosion when run on large systems. For example, NICE took 30 hours to
model check a network with two switches, two hosts, the MAC-learning
control program (98 LoC), and five concurrent
messages between the hosts~\cite{nice}. We chose to avoid state-space explosion by analyzing logs
post-hoc, after they have been observed in production.

Program slicing~\cite{weiser1981program} is a technique for finding the
minimal subset
of a program that could possibly affect (either through control flow or data
flow) the result of a particular line of code.
Our technique can be viewed in this light as a form of program slicing, except
that our `program' is a distributed event schedule,
and the end result we are interested in is the network misconfiguration at the end of the
execution.

% ----------------------------------------- %
%          OLD TEXT
\eat{

\colin{Reviewer OD: read Knowledge Plane For The
Internet~\cite{Clark:2003:KPI:863955.863957}}

\colin{Reviewer OA: tools already exist that systematically exercise the
behavior of a good part of the network (if not the whole network).}

%-- Program Slicing --

The delta debugging algorithm~\cite{Zeller:2002:SIF:506201.506206} seeks to solve
a problem that is exactly analogous to ours on a single machine: given input that causes a test case
to fail, what is the minimum subset of the input that still produces the failure?
We apply the same reasoning to a distributed system.

%-- Deterministic Replay (OFRewind) --

Deterministic replay techniques such as OFRewind~\cite{ofrewind}
are designed to allow developers to interactively prune
the inputs that lead up to errant behavior. We present an algorithm that
automates this process.

%-- Model checking (NICE): --

NICE~\cite{nice} combines model checking with concolic execution
to enumerate all possible code paths taken by control software (NOX)
and identify concrete inputs (\eg{} control message orderings) that cause
the network to enter invalid configurations. NICE works well for small
control programs and a small number of machines, but suffers from exponential
state explosion when run on large systems. For example, NICE took 30 hours to
model check a network with two switches, two hosts, the MAC-learning
control program (98 LoC), and five concurrent
messages between the hosts~\cite{nice}. We chose to avoid state-space explosion by analyzing logs
{\em post-hoc}, after they have been observed in production.

%-- Invariant Checking? --

\eat{
Invariant checking tools such as Anteater~\cite{anteater} and HSA~\cite{hsa}
detect problems in the dataplane. We leverage invariant checking tools
to distinguish inputs that are necessary for reproducing a given invariant violation.
}

%-- Root cause analysis? --

Root cause analysis techniques~\cite{577079} seek to identify the minimum set of failed
components (\eg{} link failures) needed to explain a collection of alarms. Rather than
focusing on individual component failures, we seek to minimize inputs that affect the behavior
of the overall distributed system.

%-- Distributed Systems debuggers --

Pip~\cite{pip} is a framework for instrumenting general-purpose distributed systems
with code to record, display, and check invariants on causal paths throughout
live executions. \Simulator{} observes the causal behavior of the
distributed system in a simulated environment, enabling us to iteratively prune extraneous input events.

% ------------ OLD -----------

\eat{
This work extends a growing literature on troubleshooting tools for
Software-Defined Networks.

The work most closely related to ours is NICE~\cite{nice}. NICE combines concolic execution
and model checking to automate the process of testing NOX applications. This enables one to catch bugs before
they are deployed.

Our approach and NICE complement each other in several ways.  First, NICE's systematic exploration of failure orderings
is potentially of great use for finding corner-case errors, which we could then add to our regression suite. NICE may also be applied directly to the code-base of the SDN platform, but in the case that only a subset
of all possible code-paths in the SDN platform can be model-checked due to state-space explosion;
our mechanisms allows users to troubleshoot errors
{\it post-hoc} after they are observed in production, so we can find bugs that might be missed due to truncating the state-space exploration.
In complement to NICE, correspondence checking helps developers isolate the
specific component of the SDN platform responsible for an error, without needing to specify invariants.

Focusing on the physical network, Anteater~\cite{anteater} and HSA~\cite{hsa}
are alternative approaches to statically checking invariants in the
configuration of switches and routers. Both take take as input a snapshot of
the FIB of each network device. To check invariants, Anteater generates a set of constraint functions and feeds them through a SAT
solver, while HSA defines an algebra for virtual packets and
their transformation through the network. We leverage the HSA work in \projectname{}, and our simulator allows us to detect correctness violations not just in a given set of tables but what tables are produced by a wide range of scenarios. \

Also focusing on the physical network, OFRewind~\cite{ofrewind} develops
record and replay techniques for the control plane of OpenFlow networks.
Unlike \simulator, OFRewind focuses specifically on OpenFlow
interactions, while we focus on more course-grained replay of
failures and topology changes. Running replay within a simulator also allows
us to manually modify the execution of the system, rather than playing a
static recording.

Another line of work aims to prevent bugs from being introduced in the first
place. Frenetic~\cite{frenetic} presents a language-based approach to building
robust SDN applications. By providing a specialized programming model,
 Frenetic helps developers avoid writing common classes of
bugs, such as `composition errors' where installed flow entries override each other.
Reitblatt et al.~\cite{consistentupdates} developed a technique for ensuring
consistent routing updates, guaranteeing that all switches in the network either route
a given packet under the new configuration or under the old configuration,
but not both. These abstractions are valuable for preventing common, difficult errors
in platform logic.

Several other network simulators exist for testing SDN controllers. Mininet is a
platform for emulating OpenFlow switches and hosts within a single
 VM~\cite{Lantz:2010:NLR:1868447.1868466}. The ns-series of network simulators
provides a general framework for testing new protocols, topologies,
and traffic mixes~\cite{ns3}. We found that these existing simulators did
not provide sufficient support for the corner-cases situations which are the
focus of our work, such as failures and VM migration.

Many of our ideas originate from the literature on troubleshooting general
distributed systems. WiDS checker introduced the notion of recording
production executions to be later replayed and verified in a controlled simulation~\cite{Liu07widschecker:}.
Pip~\cite{pip} defines a DSL and collection of annotation tools to
reason about causal paths throughout the execution of the
distributed system. Finally, end-to-end tracing
frameworks such as X-Trace~\cite{Fonseca:2007:XPN:1973430.1973450} and
Pinpoint~\cite{Chen02pinpoint:problem} provide a framework for tracing requests throughout
a distributed system in order to infer correctness errors between layers and
across components. Our work solves a more constrained problem; we leverage
the structure of the SDN stack to enable a simple notion of platform
correctness. In addition, these systems assume that invariants should hold at
all times; we observe that in an eventually-consistent system such as SDN,
transient correctness violations are inevitable. We built \simulator{} to help troubleshooters
differentiate ephemeral from persistent errors.

% If we manage to run multiple applications by Monday, we should cite papers
% on consistency and cross-layer debugging:
%X-Trace~\cite{xtrace}
% Vector Clocks
% Onix
% Virtualization definitely won't happen by Monday. But, papers include
% Martin's presto '10 paper 'Virtualizaing the Network Forwarding Plane'

}
}
