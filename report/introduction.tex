Software-defined networking (SDN) is an emerging paradigm that seeks to redefine
the structure of the network control plane for increased flexilibity and
simplified management. Traditional packet-switched networks typically couple
data-plane forwarding and control-plane decision making. Each switch and router
both forwards packets and independently decides on the forwarding, failure
recovery, and access control, partially coordinated by distributed protocols.
SDN breaks this coupling and enables the control plane decision logic to evolve
independently of the forwarding. Additionally, it introduces the notion of a
Network Operating System (NOS) that provides a centralized view of the network
state to applications and enables them to specify the intended behavior on top
of higher level abstractions.

In OpenFlow~\cite{openflow}, a popular emerging SDN implementation, all
control-plane decisions are made by a logically-centralized
controller\footnote{In practice, the controller is typically sharded and/or
replicated across multiple control servers for scalability and durability}. The
controller pushes forwarding decisions to `dumb' switches who simply store a
bitmask of the packet header and the associated forwarding decision and apply it
to later packets. Primarily because of its flexibility and ease of use, OpenFlow
is quickly becoming widely adopted in both industry~\cite{nicirahomepage,
bigswitch} and academia~\cite{nox, pox, ethane}.

Modern OpenFlow controllers are often architected with a three layer approach
that follows the NOS approach outlined above: a Network Operating System (NOS)
layer manages switch state, keeping track of which switches are members of the
network and pushing forwarding decisions to the switches; a virtualization layer
wraps the switch state in a programmable abstraction for administrator
configuration; and a control application, written by a network administrator,
expresses the administrator's network requirements in terms of the
virtualization layer abstraction. When a switch observes a packet that does not
match a forwarding rule, it relays the packet to a control server running an
NOS. The NOS first updates its model of the global network configuration. The
virtualization layer then translates this change into an abstract representation
of the network state, for example, in generating object representations of each
switch whose configuration will be replicated by the NOS on to the physical
network. After the virtualization layer is updated, one or more hosted control
applications are notified of the event. Lastly, a control application decides
how to handle the packet, informs the NOS of the decision, and the NOS pushes
the new configuration to the network. \andi{We need a figure here that
illustrates the chain of events.}

This structured approach isolates the control applications from the specifics
and hardware details of the underlying network. It also decouples the control
forwarding decision logic from the mechanism of distributing the state in the
network. This facilitates a more concise, side-effect free spefication of
intedended application behavior. Ideally, each application is reduced to a
state-less, side-effect free function $f(view) \rightarrow configuration$, that is
cleanly specified and easy to validate.

However, this simplification comes at a price. The complex platform
and the multiple indirections and abstractions increase the 'distance to
the metal'. This makes it difficult to correlate observed low-level behavior
(e.g., a specific forwarding rule in a switch) with its high-level specification
(a forwarding decision specified in terms of a virtual topology.) Also, the
complexity of the NOS itself gives rise to bugs of there own that can be
difficult to localize and troubleshoot. 

We now discuss three classes of errors that can be introduced in turn:

\noindent {\it Dynamic misconfigurations.} Many traditional networks employ static configuration
for access control and management, and only dynamically configure forwarding.
As a result, a single verification of the state of the network can catch most potential bugs. 
In contrast, software-defined networks can be more dynamic, and blur the distiction between configuration
and forwarding state. They may, e.g., perform an admission check and peform a forwarding decission, 
thus creating new FIB entries, on the arrival of each new flow.  For verification to be
effective in this environment, checks must span set of possible output configurations for a given control
application.

\eat{ Discuss this later.
Moreover, there exist system executions in SDNs which result in error conditions that will never 
detected by a static checker. \colin{TODO: describe Justine's cool overlapping flow entries example}
}
\noindent {\it Correspondence violations.}  Each layer of the SDN architecture
maintains its own model of the network state; the switches maintain forwarding
rules in TCAMs, the NOS represents the state of the switches in a graph-like data
structure, and control applications typically operate on 
a virtualized version of that graph. In theory, each layer's model should eventually
converge to a point where there is a one-to-one mapping 
between the other layers' models, but this does not always hold in practice.

\eat{
This class of errors is particularly pernicious, as the 
virtualization layer of the NOS is often quite complex. Consider for example
that an entire datacenter network supporting 10s of thousands of hosts may be
treated as a single logical switch. When the control application makes a
configuration change to the logical switch, the NOS needs to carefully map this
configuration onto multiple physical switches along the path. Any mismapping in this
process may prevent the network from ever behaving exactly as the application intended.
}
\noindent {\it Distribution Consistency violations.} Production SDN deployments distribute the NOS
 across multiple servers to achieve fault-tolerance
and scalability. Distributed control gives rise to the same types of coordination
and consistency bugs that affect general distributed systems. 
\eat{ Section 2.
We note that two distinct classes of consistency violations occur in SDNs.
Transient consistency violations depend on the ordering of events in the system. For example,
if the  NOS pushes out a new forwarding policy
which involves multiple switches, in-flight packets may enter a
temporary forwarding loop while each of the switches converge on the new policy.
The effects of transient
consistency violations are noticeable distinct from persistent violations. Conder for example, \colin{some example}
}
%Although the SDN stack does eventually converge to
%a consistent view of the network, such errors may present a significant
%impedance to smooth operation if such errors occur often. 

%\justine{is our goal to detect transient errors? it seems like detecting and 
%resolving transient errors is a good goal for a second project} \colin{Agreed.
%Both transient and persistent errors are important
%problems. But you go about dealing with them in very different ways. Not sure
%if the  distinction is valuable here (there are a million different
%distinctions we could make...)}


We present an overview of typical bugs encountered in large-scale, production SDNs in Section~\ref{sec:bugs}.
We derived these classifications based on bugs reports and discussions with engineers at Nicira Networks~\cite{nicira}, 
a major SDN start-up;
to the best of our knowledge this is the first publication of such data in the research community.
When encountering one of these problems,
operators currently use ad-hoc troubleshooting tools such as ping or traceroute.
If these techniques fail to isolate the problem, operators often resort to
manually gathering log files from each layer of the system and painstakingly
correlating the effects of a bug to identify its root cause.

To simplify the debugging process, we present \projectname{}, a comprehensive debugger for the SDN stack.
To this end, \projectname{} leverages existing techniques wherever possible, while improving the state of affairs
with three new techniques for SDN debugging. We utilize a static physical network invariant
checker~\cite{anteater} to detect bugs in any single configuration of the
physical network. To capture the dynamic nature of software-defined networks,
we build off of a range of input generation techniques including fuzzing, tracing, record and
replay, and symbolic execution for effective exploration of a given control
application's configuration space. 
As a comprehensive solution, \projectname{} builds off of these existing techniques to present
 three new techniques for SDN debugging:

\noindent {\it Dynamic invariant checking}:  Because the arrival of each new flow may update a FIB entry in a switch,
 new bugs may
manifest at any time, dependent on the flows that have traversed the network. \projectname{} `fuzzes' the network with 
test traffic in multiple rounds to test multiple network configurations.

\noindent {\it Cross-layer correlation}: Mistranslations between layers of the SDN stack can create bugs, some which
may even be undetectable by looking at raw FIB entries. \projectname{} walks the data structures representing network state
at each layer of the NOS to ensure consistency.

\noindent {\it Distributed controller fuzzing}: When a network is controlled by a distributed controllers, consistency
errors can arise. \projectname{} enables the detection of such errors, by spawning multiple instance of controller
applications and replaying inputs, while fuzzing the timing of state distribution events between the controllers. When forwarding
configuration computed by the controllers changes in response to a timing change, this is an indication of a consistency
bug in the controllers.

\eat{
forwarding 

servers makes a complex distributed system (managing the state at hundreds or thousands of switches) even more complex, as even the management layer itself may become inconsistent
\projectname{}...\justine{colin, you're going to have to say something here}
\colin{I'm not sure that we want to have three techniques just for the sake of
having three techniques... system-wide verification is definitely something
that we'll need in the future, but my ideas are so nascent that I'm not sure
it's worth putting them on paper...}
}
%
%First, distributed system designers model inter-node communication as a black box, whose failure modes are an independent class of failures from the system itself.
%Thus, distributed systems makes guarantees that hold only ``when the network recovers from partition.''
%In SDN, however, the failed system {\it is} the network: loss of
%communication and system failure are one and the same. This implies an
%entirely different class of failure-modes.
%
%\justine{How does this come across in CLINT?} \colin{Right now, it
%doesn't. However, when we start dealing with consistency bugs, we will
%need to treat the switches as a first class entity in the distributed
%system. In particular, we will need to make sure that they support
%vector clocks to be able to infer a global ordering of events.}
%
% \justine{wait, is this a debugging technique or something you would do to
% stop consistency bugs from happening?}
% \colin{Vector clocks are a debugging technique. Without them, you have no way
% to reason about causation (the order in which events occur)}
%
% \justine{I would say that the second here is the most important insight}
%Second, SDN provides significant domain-specific context to inform the debugging process.
%The  SDN architecture provides well-defined interfaces whose correct interactions can be verified; \eg{} that the state of a switch and the state of its representation in the NOS should be consistent.
%Further, networking itself requires specific properties of the the network graph, the state of the switches, \etc{}; for example, the network graph should always be acyclic.
%These domain-specific invariants do not need to be specified by the programmer, but instead should be ``baked-in'' to the SDN debugging process.

\eat{redundant?
This paper makes two research contributions. First, it provides the first
analysis of common system errors seen in production SDN deployments. In
particular, through discussions with Nicira we show that bugs {\it within} the platform
itself pose a significant problem for developers and operators of software-defined
networks. Second, we present \projectname{}, a comprehensive debugger to detect and isolate bugs in SDNs.
\projectname{} provides three novel methodologies for ameliorating the difficulty of
troubleshooting such errors: dynamic invariant checking, cross-layer
correlation, and cross-server verification.
}

The rest of this paper is organized as follows. In \S\ref{sec:bugs}, we present an overview of common bugs
encountered by users of production SDN deployments. In \S\ref{sec:approach}, we present our approach to
detecting and isolating these bugs. In \S\ref{sec:architecture}, we present the
architecture of our implementation, \projectname{}, and in
\S\ref{sec:evaluation} we evaluate \projectname{}. Finally, in \S\ref{sec:future_work} we discuss our plans
for future work, in \S\ref{sec:related_work} we discuss related work, and in \S\ref{sec:conclusion} we conclude.
