
\colin{Maybe some floof about the proliferation SDN?}

Unlike in a traditional network, where each switch and router is independently responsible for
its own forwarding decisions, failure recovery, and access control, all management
decisions in a software-defined network are made by a logically-centralized controller. When a
switch observes a packet that does not match a forwarding rule, 
it relays the packet to a control server running a ``network operating system''
(NOS). The NOS begins updating its model of the global network configuration.
The NOS then notifies one or more network management applications of the event. Finally, the management
application decides how to handle the packet, informs the NOS of the decision,
and the NOS pushes the new configuration to the switch.

This modularity gives rise to a class of system errors which do not exist in
traditional networks: {\it semantic mismatches} \colin{Better name?}. Each layer of the architecture
maintains its own model of the network state; the switches maintain forwarding
rules in TCAMs, the NOS represents the state of the switches in a graph-like data
structure, and the control application typically operates on 
a virtualized version of that graph. In theory, each layer's model should eventually
converge to a point where there is a one-to-one mapping 
between the other layers' models, but this does not always hold in practice.

Semantic mismatches may manifest themlselves in one of two ways.

\noindent {\it Transient errors} may be triggered by a particular ordering of
events in the system. For example, if the NOS pushes a new forwarding policy
to the network which involves multiple switches, in-flight packets may enter a
temporary forwarding loop while the switches converge on the new policy.
\colin{More examples? Would be nice to have an example that depends on the
order in which packets arrive.} Although the SDN stack does eventually converge to
a consistent view of the network, such errors may present a significant
impedance to smooth operation if such errors occur often. 

\noindent {\bf Persistent errors} may be caused by software bugs and
byzantine hardware failures. Software bugs are particularly common in
production software-defined network deplyoments, for two reasons. First, the
virtualization layer of the NOS is often quite complex. Consider for example
that an entire datacenter network supporting 10s of thousands of hosts may be
treated as a single logical switch. When the control application makes a
configuration change to the logical switch, the NOS needs to carefully map this
configuration onto mulitple physical switches along the path. Second,
production SDN deployments distribute the NOS across multiple servers to achieve fault-tolerance
and scalability. Distributed control gives rise to the same types of coordination
and consistency bugs that affect general distributed systems.

From our experience talking to developers of software-defined networks
~cite{Nicira}, we find that there is a serious need for systematic troubleshooting
tools and techniques for the software-defined networking stack. When a problem occurs,
operators often resort to ad-hoc troubleshooting techniques such as traceroute or ping.
If these techniques fail to isolate the problem, operators often resort to
manually examining log files from each layer of the system, and painstakingly
correlating the effects of the bug to identify the root cause of a problem.

In this work we present a new technique for debugging the software-defined
netwoking stack: cross-layer analysis. Our prototype system, CLINT, interposes 
on the layers of the SDN stack to enable isolated invariant checking of individual
components.

An SDN debugger must be general enough to address all of the above problems, and provide programmers
with the ability to (i) detect that a problem exists, (ii) identify the layer / component / line of code
responsible for the problem, and (iii) reproduce the execution that triggered the error.

In general, distributed systems debugging techniques lie somewhere on a spectrum between
{\it static checking} and {\it interactive replay}. Our first goal is to find
the point on this spectrum most appropriate for SDN. 

\noindent {\bf Static-checkers} \cite{anteater} excel at {\it detecting} that problems
exist. However, SDNs are far too dynamic for pure static checking; one would
essentially need to take snapshots at every state change. Moreover,
            knowing {\it that} a problem exists does not necessarily help isolate its
            cause.

            \noindent {\bf Tracing frameworks} \cite{x-trace} excel at pinpointing the cause of an
            error. However, the space of possible traces is intractable, so it can be very
            difficult to produce the input that causes the error to arise in the first
            place.

            \noindent {\bf Interactive debuggers} facilitate intuition and shorten the
            debugging process. However, true interactivity is extremely difficult to
            obtain in a distributed system. 
            %That said, there may be hope with SDN:
            %the controller can simply refrain from installing flow rules in the switches
            %such that {\it all } packets are sent to the
            %centralized controller. Nevertheless, an interactive debugger would still
            %have difficulty detecting problems and reproducing problematic
            %executions. Moreover, 
            Moreover, while interactive debuggers are
            well-suited for sequential
            computations, networks are inherently event-based.

            Although distributed systems debugging is well explored,
            software-defined networks differ in a number of ways.

            \justine{How does this come across in CLINT?} \colin{Right now, it
                doesn't. However, when we start dealing with consistency bugs, we will
                    need to treat the switches as a first class entity in the distributed
                    system. In particular, we will need to make sure that they support
                    vector clocks to be able to infer a global ordering of events.}
                    First, distributed system designers model inter-node communication as a black box, whose failure modes are an independent class of failures from the system itself.
                    Thus, distributed systems makes guarantees that hold only ``when the network recovers from partition.''
                    In SDN, however, the failed system {\it is} the network: loss of
                    communication and system failure are one and the same. This implies an
                    entirely different class of failure-modes.

                    Second, SDN provides significant domain-specific context to inform the debugging process.
                    The  SDN architecture provides well-defined interfaces whose correct interactions can be verified; \eg{} that the state of a switch and the state of its representation in the NOS should be consistent.
                    Further, networking itself requires specific properties of the the network graph, the state of the switches, \etc{}; for example, the network graph should always be acyclic.
                    These domain-specific invariants do not need to be specified by the programmer, but instead should be ``baked-in'' to the SDN debugging process.

                    CLINT does\ldots

                    This work makes to research contributions. First, it provides the first
                    published view into the types and relative severity of problems seen in production SDN deployments. In particular, by
                    examining bug reports from Onix, we show that bugs {\it within} the platform
                    itself, especially those caused by inconsistencies between distinct control server
                    instances, pose a significant problem for operators of software-defined
                    networks. Second, we present two methodologies for ameliorating the difficulty of
                    detecting and isolating such errors: cross-layer analysis, and dynamic invariant checking.
