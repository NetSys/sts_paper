Software-defined networking (SDN) is an emerging paradigm that redefines
the structure of the network control plane for increased flexibility and
simplified management. In traditional networks, data-plane forwarding is
typically coupled to control-plane decision making; in addition to forwarding
packets, each switch and router independently computes routing, failure
recovery, and access control decisions through distributed coordination protocols.
SDN breaks this coupling and enables the control plane decision logic to evolve
independently of the forwarding. Additionally, it introduces the notion of a
Network Operating System (NOS) that provides a centralized view of the network
state to applications and enables them to specify the intended behavior on top
of higher level abstractions. Primarily because of its flexibility and ease of use, SDN
is quickly becoming widely adopted in both industry~\cite{nicirahomepage,
bigswitch} and academia~\cite{nox, pox, ethane}.

Modern SDN controllers are architected with a three layer approach: the NOS
layer is responsible for tracking switch state and pushing configuration
changes to the switches themselves; a virtualization layer
wraps the state of the network in a programmable abstraction; and a control
application written by network administrators 
expresses the network behavior in terms of the virtualization layer abstraction.
When a switch observes a packet that does not match a forwarding rule, it relays the
packet to the NOS layer of a control server.
The NOS then updates its model of the global network configuration. The
virtualization layer translates this change into its own network model. 
Finally, one or more hosted control applications are notified of the event, a
control decision is made, the NOS is informed of the decision, and the NOS
pushes the new configuration to the network.
\eat{ Agreed, but maybe not in the intro
\andi{We need a figure here that
illustrates the chain of events.}
}

This structured approach isolates control applications from the specifics
of the underlying network. This facilitates a concise specification of
intended network behavior. Ideally, each application is reduced to a
state-less, side-effect free function $f(view) \rightarrow configuration$, that is
easy to validate.

However, this simplification comes at a price. The 
many indirections and abstractions in the platform increase the application's 'distance to
the metal', making it difficult to correlate observed low-level behavior
(\eg{} a specific forwarding rule in a switch) with its high-level specification
(a forwarding decision specified in terms of a virtual topology.) In addition, the
complexity of the NOS itself gives rise to bugs of there own that can be
difficult to localize and troubleshoot. 

Based on bug reports and discussions at a major SDN start-up; we find that
there is a serious need for system troubleshooting techniques for the SDN
stack. When encountering one of these problems,
operators currently use ad-hoc troubleshooting tools such as ping or traceroute.
If these techniques fail to isolate the problem, operators often resort to
manually gathering log files from each layer of the system and painstakingly
correlating the effects of a bug to identify its root cause.

To simplify the debugging process, we present \projectname{}, a comprehensive debugger for the SDN stack.
\projectname{} leverages existing techniques wherever possible, and improves the state of affairs
with two new techniques for SDN debugging.

\eat{We utilize a static physical network invariant
checker~\cite{anteater} to detect bugs in any single configuration of the
physical network. To capture the dynamic nature of software-defined networks,
we build off of a range of input generation techniques including fuzzing, tracing, record and
replay, and symbolic execution for effective exploration of a given control
application's configuration space. 
As a comprehensive solution, \projectname{} builds off of these existing techniques to present
 three new techniques for SDN debugging:
}

\noindent {\it Dynamic invariant checking}:  Because the arrival of each new flow may update a FIB entry in a switch,
 errors may be dependent on the flows that have previously traversed the network. \projectname{} `fuzzes' the network with 
test traffic in multiple rounds to test multiple network configurations.

\noindent {\it Cross-layer correlation}: Mistranslations between layers of the SDN stack can create bugs, some which
may be undetectable by looking at raw FIB entries. \projectname{}
translates between between each abstraction layers' representation of the
network state to verify a correspondence between the application's intentions
and the physical configuration.

\eat{ If we want three contributions, mention isolated testing (per-layer
verification)
\noindent {\it Distributed controller fuzzing}: When a network is controlled by a distributed controllers, consistency
errors can arise. \projectname{} enables the detection of such errors, by spawning multiple instance of controller
applications and replaying inputs, while fuzzing the timing of state distribution events between the controllers. When forwarding
configuration computed by the controllers changes in response to a timing change, this is an indication of a consistency
bug in the controllers.
}

\eat{
forwarding 

servers makes a complex distributed system (managing the state at hundreds or thousands of switches) even more complex, as even the management layer itself may become inconsistent
\projectname{}...\justine{colin, you're going to have to say something here}
\colin{I'm not sure that we want to have three techniques just for the sake of
having three techniques... system-wide verification is definitely something
that we'll need in the future, but my ideas are so nascent that I'm not sure
it's worth putting them on paper...}
}
%
%First, distributed system designers model inter-node communication as a black box, whose failure modes are an independent class of failures from the system itself.
%Thus, distributed systems makes guarantees that hold only ``when the network recovers from partition.''
%In SDN, however, the failed system {\it is} the network: loss of
%communication and system failure are one and the same. This implies an
%entirely different class of failure-modes.
%
%\justine{How does this come across in CLINT?} \colin{Right now, it
%doesn't. However, when we start dealing with consistency bugs, we will
%need to treat the switches as a first class entity in the distributed
%system. In particular, we will need to make sure that they support
%vector clocks to be able to infer a global ordering of events.}
%
% \justine{wait, is this a debugging technique or something you would do to
% stop consistency bugs from happening?}
% \colin{Vector clocks are a debugging technique. Without them, you have no way
% to reason about causation (the order in which events occur)}
%
% \justine{I would say that the second here is the most important insight}
%Second, SDN provides significant domain-specific context to inform the debugging process.
%The  SDN architecture provides well-defined interfaces whose correct interactions can be verified; \eg{} that the state of a switch and the state of its representation in the NOS should be consistent.
%Further, networking itself requires specific properties of the the network graph, the state of the switches, \etc{}; for example, the network graph should always be acyclic.
%These domain-specific invariants do not need to be specified by the programmer, but instead should be ``baked-in'' to the SDN debugging process.

\eat{redundant?
This paper makes two research contributions. First, it provides the first
analysis of common system errors seen in production SDN deployments. In
particular, through discussions with Nicira we show that bugs {\it within} the platform
itself pose a significant problem for developers and operators of software-defined
networks. Second, we present \projectname{}, a comprehensive debugger to detect and isolate bugs in SDNs.
\projectname{} provides three novel methodologies for ameliorating the difficulty of
troubleshooting such errors: dynamic invariant checking, cross-layer
correlation, and cross-server verification.
}

The rest of this paper is organized as follows. In \S\ref{sec:bug_analysis}, we present an overview of common bugs
encountered by users of production SDN deployments. In \S\ref{sec:approach}, we present our approach to
detecting and isolating these bugs. In \S\ref{sec:architecture}, we present the
architecture of our implementation, \projectname{}, and in
\S\ref{sec:evaluation} we evaluate \projectname{}. Finally, in \S\ref{sec:future_work} we discuss our plans
for future work, in \S\ref{sec:related_work} we discuss related work, and in \S\ref{sec:conclusion} we conclude.
