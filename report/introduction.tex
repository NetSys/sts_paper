Software-defined networking is an emerging paradigm for managing the
configuration of network devices. Primarily because of its flexibility and ease of use, SDN is
quickly becoming widely adopted in both industry and academia. \colin{Fluff!}

In a traditional network, each switch and router is independently responsible for
its own forwarding decisions, failure recovery, and access control. In
contrast, all control-plane decisions in a software-defined network are made by
a logically-centralized controller. When a
switch observes a packet that does not match a forwarding rule, 
it relays the packet to a control server running a ``network operating system''
(NOS). The `device-driver` layer of the NOS first updates its model of the global network configuration.
The virtualization layer then translates this change into an abstract 
representation of the network state in order to hide low-level details. Next, one or more hosted control
applications are notified of the event. Lastly, the
control application decides how to handle the packet, informs the NOS of the decision,
and the NOS pushes the new configuration to the network.

These abstractions give rise to three classes of system errors. We describe these in turn.

\noindent {\it Dynamic misconfigurations.} Many traditional networks are statically configured.
As a result, a single verification of the state of the network can catch most potential bugs. 
In contrast, software-defined networks typically state quite rapidly. For verification to be
effective in this environment, checks must span set of possible output configurations for a given control
application.

Moreover, there exist system executions in SDNs which result in error conditions that will never 
detected by a static checker. \colin{TODO: describe Justine's cool overlapping flow entries example}

\noindent {\it Correspondance violations.}  Each layer of the SDN architecture
maintains its own model of the network state; the switches maintain forwarding
rules in TCAMs, the NOS represents the state of the switches in a graph-like data
structure, and control applications typically operate on 
a virtualized version of that graph. In theory, each layer's model should eventually
converge to a point where there is a one-to-one mapping 
between the other layers' models, but this does not always hold in practice.

This class of errors is particularly pernicious, as the 
virtualization layer of the NOS is often quite complex. Consider for example
that an entire datacenter network supporting 10s of thousands of hosts may be
treated as a single logical switch. When the control application makes a
configuration change to the logical switch, the NOS needs to carefully map this
configuration onto mulitple physical switches along the path. Any mismapping in this
process may prevent the network from ever behaving exactly as the application intended.

\noindent {\it Consistency violations.} Production SDN deployments distribute the NOS
 across multiple servers to achieve fault-tolerance
and scalability. Distributed control gives rise to the same types of coordination
and consistency bugs that affect general distributed systems. \colin{Even in a single-controller
topology, there are still consistency violations between the controller and the switches. We don't
normally think of the network as a first-class entity in the distributed system, but in our case it
is!}

We note that two distinct classes of consistency violations occur in SDNs.
Transient consistency violations depend on the ordering of events in the system. For example,
if the  NOS pushes out a new forwarding policy
which involves multiple switches, in-flight packets may enter a
temporary forwarding loop while each of the switches converge on the new policy.
\colin{Would be nice to have an example that depends on the
order in which packets arrive.} 
The effects of transient
consistency violations are noticeable distinct from persistent violations. Conder for example, \colin{some example}

%Although the SDN stack does eventually converge to
%a consistent view of the network, such errors may present a significant
%impedance to smooth operation if such errors occur often. 

%\justine{is our goal to detect transient errors? it seems like detecting and 
%resolving transient errors is a good goal for a second project} \colin{Agreed.
%Both transient and persistent errors are important
%problems. But you go about dealing with them in very different ways. Not sure
%if the  distinction is valuable here (there are a million different
%distinctions we could make...)}

\colin{Need a word to describe isolation of a bug to a particular control
server, re: replication. `cross-component` seems way too ill-defined to me.
e.g., is an object a component of the system? A method? A variable? etc. }

Based on discussions with developers of a network operating system
\cite{Nicira}, we argue that there is a serious need for systematic troubleshooting
tools and techniques for the SDN stack. When a problem occurs,
operators currently use ad-hoc troubleshooting tools such as ping or traceroute.
If these techniques fail to isolate the problem, operators often resort to
manually gathering log files from each layer of the system and painstakingly
correlating the effects of a bug to identify its root cause.

In this work we present two new techniques for debugging the SDN stack:
dynamic invariant checking and cross-layer correlation. Our prototype system,
\projectname{}, demonstrates that these techniques are effective at detecting and
isolating common errors in SDN deployments.

Our goal in designing \projectname{} has been to provide a comprehensive solution to
troubleshooting bugs in the SDN stack. To this end, we
leverage existing techniques whenever possible. We utilize a static dataplane invariant
checker \cite{anteater} to detect bugs in any single configuration of the
physical network. To capture the dynamic nature of software-defined networks,
we also support a range of input generation techniques including fuzzing, tracing, record and
replay, and symbolic execution for effective exploration of a given control
application's configuration space. Finally, we leverage the layered nature of the
SDN stack to enable semantic checks of the network configuration \colin{by
`semantic`, I mean `the network should do what the application tells it to`.
Need to reword.} as
well as fine-grained root-cause analysis.

\colin{Quick discussion of related work? Especially relation to debuggers for
general distributed systems..}

%Although distributed systems debugging is well explored,
%software-defined networks differ in a number of ways.
%
%
%First, distributed system designers model inter-node communication as a black box, whose failure modes are an independent class of failures from the system itself.
%Thus, distributed systems makes guarantees that hold only ``when the network recovers from partition.''
%In SDN, however, the failed system {\it is} the network: loss of
%communication and system failure are one and the same. This implies an
%entirely different class of failure-modes.
%
%\justine{How does this come across in CLINT?} \colin{Right now, it
%doesn't. However, when we start dealing with consistency bugs, we will
%need to treat the switches as a first class entity in the distributed
%system. In particular, we will need to make sure that they support
%vector clocks to be able to infer a global ordering of events.}
%
% \justine{wait, is this a debugging technique or something you would do to
% stop consistency bugs from happening?}
% \colin{Vector clocks are a debugging technique. Without them, you have no way
% to reason about causation (the order in which events occur)}
%
% \justine{I would say that the second here is the most important insight}
%Second, SDN provides significant domain-specific context to inform the debugging process.
%The  SDN architecture provides well-defined interfaces whose correct interactions can be verified; \eg{} that the state of a switch and the state of its representation in the NOS should be consistent.
%Further, networking itself requires specific properties of the the network graph, the state of the switches, \etc{}; for example, the network graph should always be acyclic.
%These domain-specific invariants do not need to be specified by the programmer, but instead should be ``baked-in'' to the SDN debugging process.

This paper makes two research contributions. First, it provides the first
analysis of common system errors seen in production SDN deployments. In
particular, through discussions with Nicira we show that bugs {\it within} the platform
itself pose a significant problem for developers and operators of software-defined
networks. Second, we present two methodologies for ameliorating the difficulty of
troubleshooting such errors: dynamic invariant checking and cross-layer
correlation.
