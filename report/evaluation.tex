Three forms of evaluation:

\subsection{Did our system find bugs in third-party applications?}
\colin{Nope, chuck testa}

\subsection{Is our system sufficiently powerful to capture the kinds of bugs we want
it to?}

Totally okay to construct the bugs here. Three versions of this, depending on what we get done by Monday:

i. those normally caught by Anteater 

ii. those that wouldn't be caught by Anteater.

Would be excellent to describe Justine's `Overlapping flows, partially
installed` example in depth.

iii. those described by Nicira

Namely, consistency bugs, virtualization bugs (not going to happen by Monday),
and poorly implemented state-machine bugs (also not going to happen by
Monday).

\colin{Insert nice diagrams here!}

\subsection{Is the overhead of our system reasonable?}

There is already a nice evaluation of Anteater. Something like 400 seconds for
a large network.

Now we're running Anteater in a
loop. Is this going to take rediculously long? We hope not.

\begin{figure}[t]
    \centering
    \includegraphics[width=3in]{../graphs/mock_overhead_graph.jpg}
    \caption[]{\label{fig:loop} Our lovely graph showing that execution time
    is not too bad\vspace{-10pt}} 
\end{figure}

Two orthogonal ways to ameliorate the overhead. First, you could intellegintly choose
when to run Anteater, so that you don't have to run it for every possible
configuration, while still getting good coverage of the
configuration space. Second, you could make Anteater faster, perhaps by only
getting `best-effort` exploration of the invariant checks rather than
full-blown proofs that the invariant holds. Also, Brighten mentioned that the
current version of Anteater runs significantly faster.
