% Feedback from Ali Ghodsi:
%  - Why is S/W easier to test than H/W? It's not really this point that's
%    important either: it's the centralized control plane that makes it easier
%    to test. [rebuttal: cite Urs Hoelzle]
%  - SDN is not more challenging! (read: SDN doesn't not offer a challenge)
%    Not problematic that the difficulty hasn't chanfed -- just make it clear
%    that we aren't getting farther away from the distributed nature of
%    networks.
%  - Need to NAIL the motivation:
%      - Distributed systems are fundamentally hard to troubleshoot, and
%      they're going to remain that way forever.
%      - SDN provides an opportunity to do better than traceroute! In
%      traditional networks, configuration state is spread all over the place.
%      - Not trivial in SDN (aren't running away from distributed nature)
%        It presents a new challenge: how to verify high-level policies have
%        actually been implemented correctly? => enter correspondence checking.
%      - What's the key insight here?
%  - Say what W^3 stands for?
%  - Too much boiler plate
%  - How much do the readers learn from reading the introduction
%  - "It is important to place this work in context and scope the problem we are
%   attacking" is really lecturing the reader  ...  of course context and scope of
%   any research problem is important.

Today's software-defined networking control platforms are highly complex,
distributing state between replicated
servers~\cite{floodlight},
providing isolation and resource arbitration between multiple
tenants~\cite{Casado:2010:VNF:1921151.1921162}, or
globally optimizing network utilization~\cite{urs_keynote}.
The majority of this complexity comes from
fundamentally difficult distributed systems challenges that the networking community has
relatively little experience
solving in tightly-coupled (not fully distributed) environments.
For example, as Google's Urs H$\ddot{\mathrm{o}}$elzle recently
noted at the Open Networking Summit~\cite{urs_keynote}: ``[Coordination between replicated controllers] is going to
cause some angst, and justifiably, in the industry''. As SDN control software
is used to manage larger and more sophisticated networks,
its complexity will only continue to increase.

With complexity inevitably come
bugs. To make matters worse, the number of hardware failures,
policy changes, VM migrations, and other inputs to the system that may have
triggered a given bug can be quite large in production environments. Microsoft,
for example, reports 8.5 network error events per minute per
datacenter~\cite{Greenberg:2009:VSF:1592568.1592576}.
Consequently, it is often difficult to distinguish events that are
characteristic of operating normal
conditions from those that push the network into an erroneous state.

In this paper we seek to eliminate the need to manually identify the inputs
from logs that lead
up to software faults, thereby freeing developers to debug the problematic code itself.
We present an algorithm, \simulator{} for automatically inferring the sequence
of inputs that are both necessary and sufficient for
triggering invalid configurations discovered in executions of the system.

To support \simulator{}, we have (i) detailed how execution logs are obtained
and what properties they need, (ii) developed a deterministic network simulator
to replay the execution of control software, and (iii) developed an invariant
checking algorithm for detecing errant behavior. Beyond elaborating these components, we describe a case-study
on BigSwitch's Floodlight production
controller~\cite{floodlight}, where we trigger a
corner-case failover scenario between replicated controllers and apply our
algorithm to show how the exact inputs that triggered the problem are
identified.

\Simulator{} is focused on the
system software of the SDN stack (described in the next section). While progress has been made in troubleshooting control
applications that run on top of the SDN platform~\cite{nice} and in troubleshooting the
forwarding tables in the physical switches~\cite{hsa}, we
are not aware of previous troubleshooting work that focuses on the SDN
platform itself; to the best of our knowledge, painstaking analysis of detailed logs is the current state-of-the-art in SDN platform troubleshooting.

The rest of this paper is organized as follows. In \S\ref{sec:overview},
we present an overview of the SDN stack and its failure modes.
In \S\ref{sec:approach} we present \simulator{} in detail.
We describe our experience applying \simulator{} to the Floodlight
control platform in \S\ref{sec:casestudy}, and discuss the limitations of our
approach in \S\ref{sec:discussion}. Finally, in \S\ref{sec:related_work}
we consider related related work,
and in \S\ref{sec:conclusion} we conclude.

% OSDI "Tips":
%  - Make very clear that we implemented it!
%  - Make very clear how this is an improvement over related work!
%  - What has the system's use showed about the practical implications of the
%    ideas?
%  - What have you learned from the work?
%  - How generally applicable are these ideas?
%  - What were the alternatives considered at various points?
%  - Keep forward references (to later sections) to a minimum
% This represents an improvement over the state-of-the-art SDN troubleshooting
% practices

% Feedback on version 2.0:
% not clear what you mean by "SDN platform", "policy-violation"

%\subsection{\colin{Feedback from Kay et al.}}
%\begin{itemize}
%\item Not sure that OSDI PC members will understand the subtleties of this statement: ``it is hard to tell what is broken when the goals are only implicitly expressed in the flow entries themselves.''
%\item I think we should push a little harder on emphasizing that the bugs are really nasty, and fundamental to the distributed nature of the system. "may have bugs" seems a little weak.
%\item We should make it very clear how our tools are an improvement over the status quo (log analysis). In particular, we automatically localize the (i) layer, and (ii) minimal causal set of events responsible for an error.
%\item \simulator{} does two things: find the minimal causal set, {\bf and}
%differentiates benign from pernicious. Only one of these points was clear, not
%both.
%\item The Google quote seems to indicate that the problem has already been solved!
%\item Along a similar vein, the point about ``the SDN platform will eventually
%become stable'' makes it seems like these problems aren't fundamental. But
%these problems are fundamental! How will the community view this paper in 10
%years?
%\item The reader doesn't walk away from this problem thinking ``This problem
%seems really heard!''. What are the specific bugs this framework addresses?
%Also, make it clear why this problem is important! If Pepsi knows that
%Coca-cola might be able to snoop on their traffic, we've lost huge \$\$\$.
%\item Don't frame ourselves just against log analysis. Make
%the shortcomings of the status quo very clear, and be very precise about how
%we address those shortcoming. (Why aren't traditional distributed systems
%techniques apropro? Why is static checking of a single layer insufficient? How
%is \simulator{} different than traditional replay debugging?)
%\item Would be cool to have numbers on how many log events developers have to
%step through today.
%\item Would also be cool to have numbers on how many benign policy-violations
%there are at a given point (say, in POX).
%\item ``Policy-violation'' wasn't well-defined.
%\item The introduction (prose) was too dense. Frame the problem in terms that
%the reader understands. Get them thinking about the problem right off the bat.
%Then lead them through our reasoning of how we came to the solution. Present
%a straw-man solution, and show why that doesn't work. This way
%the reader is much more engaged -- the reader should ``nod along''.
%\item What exactly are we simulating? Hardware faults? Misconfigurations?
%(answer: all of the above)
%\item How exactly does correspondence checking provide a crisp determination of the scope of a policy-violation in the
%network as well as the range of inputs that produce it? (answer: you start off
%with a customer complaint, ``A can't ping B''. Correspondence checking tells
%you all possible inputs that would experience the blackhole, and what parts of
%the network are effected)
%\item ``Stack'' connotes TCP/IP. Maybe use ``Platform'' instead?
%\item ``Fault'' connotes hardware failure. Maybe use ``Software Fault''
%\item Cite Frenetic, POX, Floodlight?
%\item What does \projectname{} stand for?
%\end{itemize}
