 - Correspondence checking interfaces: Can we reasonably claim we
   go top to bottom? What is the transfer function of a Frenetic-
   style policy function? [Kyriakos is thinking about this]

My intution is that this is almost certiainly possible at the bottom layer
of the software stack; you need to
represent the state of the physical network in some form.

The real question is: can the internal representation of policy be converted to
graphs? HSA assumes a graph format.

It might also be sufficient to say "we built a correspondence checking module
for all publicly available SDN controllers (Frentic, Floodlight, and POX), and
it worked. Here are some details on how we adapted the different controllers."

 - How do we detect inner loops vs. dropped packets? Can we support
   debugging of other constraints ('should be load balanced
   over multiple links' / 'must not be routed through Timbuktu'...)?

All externally visible behavior of the switch is captured by bisimulation
equivalence.

WRT inner loops: it seems sufficient to explicitly mark them when inspecting the propogation
graph. The question is: are we failing to capture other errant internal behavior?

Note that in order for a policy to be expressed (e.g., traffic should be load balanced
over multiple links), the entities in the physical network
relevant to that policy (multiple links) must be present in the virtual network.
If this is the case, then bisimulation equivalence should be able to capture
it.

   - Do we need bisimulation equivalence?

The answer to this question essentially depends on whether we are worried
about cases where the network does /more/ than was specified in the policy.
Consider a v-switch that broadcasts some subset of traffic rather than unicasting
it. This would be a correct simulation (all paths in the
virtual network are present in the physical network), but an incorrect
bisimulation (-> congestion)

   - Does bisimulation equivalence on a grouped graph (based on
     switches, ports, rules?) capture the right equivalence relation

This question boils down to: is our assumption that all physical
switches/ports have exactly one logical parent valid?

At least at first glance, this seems like a reasonable assumption since the
whole purpose of the virtualized network is to simplify network management.

Note that we need to run correspondence checking once for each logical network
(tentant), since there will often be a many-to-many mapping between tentants
and the physical network at this level.

    [Colin]

- Causal Inference: Detailed algorithm
   - What non-determinstic events do we capture? Are these all?

Perhaps "non-deterministic" is the wrong term. What we really mean is "external
to the system". This is essentially all inputs to the system that come from
outside. We say they're non-deterministic because we don't know /when/ these
inputs are going to be injected.

So, for a proactive controller, these inputs are:
   i.    Policy changes (triggered by humans, or perhaps scripts) 
   ii.   Host placement changes (entering/leaving/migrating)
   iii.  Network topology changes (operator rewires switches,
         or plugs in a new switch)
   iv.   Network hardware failures
   v.    Controller hardware failures 

For a reactive controller, we add:
   vi.   Traffic changes

Note that software crashes are /not/ external events.

The system might react to certain /internal/ inputs, for example timers. We
aren't interested in capturing these though, since we assume that the same
events will be reproduced by the code when run in our simulation environment.

To get a better sense of the inputs to the controller, perhaps we should take
a look at the Quantum plugin API.

   - Is the distinction between deterministic and non-determinstic
     always clear?

Certainly not. Consider the following two cases:
  i.   A controller receives a badly formatted control message, doesn't handle
       properly, and throws a null pointer exception. The controller logs this
       event before the OS cleans up the process.

  ii.  A cosmic ray comes from the sun and corrupts a bit in a server's memory.
       That portion of virtual memory belonged to the SDN control process. 
       This causes the control process to miscalculate a boolean value, and
       ends up throwing a null pointer exception. The controller logs this
       event before the OS cleans up the process.

Event i. is deterministic, whereas event ii. is completely outside of the
system's control.

However, I think we can argue that it isn't necessary to distinguish the two.
The argument goes roughly as follows:
   * Distributed systems are built under the assumption that individual
     components will fail.
   * We aren't really interested in diagnosing individual component
     failures. (e.g. if a piece of hardware fails, we aren't going to get out
     our oscilliscopes and find the exact circuit path that's going wonky)
   * We are however interested in testing the overall system's reaction to a
     particular component failure. (e.g. if the failover logic is broken, we
     want to know about it)
   -> It should be sufficient to use a "blunt hammer" to reproduce individual
     non-deterministic events. (e.g. we can just send a KILL signal to a
     control process, regardless of how the control process died in
     the actual run of the system)

   - Is 'non-deterministic events are independent' a reasonable
     assumption?
     
The answer depends on how complex we want to make our model of the network.
Consider a drunk network operator who (i) trips on an ethernet cable, and
consequently (ii) spills his drink on a switch (busting the hardware). Our
system views (i) and (ii) as causally independent, but in reality (i) caused
(ii) [or rather, (i) caused the operator to fall, which caused (ii), therefore
(i) caused (ii) by transitivity]. 

I don't think our goal is to model drunk operators though. Our goal /is/ to
test the overall system's reactions to errant behavior on one or more
individual nodes. In the case of the drunk operator, we /do/ care about how
the system reacts to (i) followed by (ii). We don't really care about figuring
out how to fix or prevent (i) and (ii) themselves -- that's the responsibility
of the hardware vendor, not the implementor of the SDN controller platform.

   - Do we need 'deterministic' events in the simulation?

Do we need to test the behavior of the system? (yes...) I think I'm
misunderstanding this question.

   - Where do we need vector clocks? Are they fundamental? Is this
     something that needs to be in the communication protocol, or can
     we just infer the relevant communication events from the trace?

Sam and I discussed this for a bit, and decided that we /do/ need vector
clocks, but probably only within the simulator. The reason is that we need to know
exactly /when/ to inject inputs to the system under simulation. If we inject
events willy-nilly, we won't get the same output every time, which is
obviously problematic if we're trying to infer which events triggered a
policy-violation.

More specifically:

We assume that external inputs to the system are /causally/ independent. But that does
not imply that there are not constraints on when we can inject the events. In
particular, if we want to produce the same output for any given run of the
simulator, we need to satisfy happens-before relations between all events
(both internal and external) in the system. *example on the whiteboard*

Lamport clocks (or physical clocks) only give a total ordering of events; they
do not say anything about the "happens-before" relation. Vector clocks capture
exactly that: they enumerate all of the happens-before constraints (i.e.
define a partial ordering) for the system execution.

So our simulation algorithm:
  - assumes that the policy-violation can be reproduced by starting from a
    snapshot and replaying the inputs in a somewhat sloppy fashion
  - On that first simulated execution, the simulator maintains vector clocks
    for all messages and gathers a set of happens-before contraints for every
    external input event.
  - On the subsequent executions, waits until all happens-before constraints
    for any given exernal input event are met before injecting it.


Note that if the policy-violation can't be reproduced in the simulated
environment, we might need to use include vector clocks in the production
network. Getting the timings right in the simulator seems too hard
otherwise...

Because we interpose on all communication channels, and because our system
proceeds in lock-step (it's single-threaded), it should be relatively easy to
check whether happens-before dependencies have been satisfied: record a
fingerprint to each message, and check in the next execution whether that
fingerprint was observed.

Note that when we prune external inputs, some set of internal events will
cease to occur in the system execution. 

The hard part is going to be tracking happens-before dependencies /within/ a single
controller (e.g. timers going off). It's not clear that this is even possible
without instrumenting the code... 

   - And how do we assume we get the trace? Maybe /this/ is where we
     need them?

We assume that:
  i.    The production system keeps a log of all external inputs (industry practice)
  ii.   The log is totally-ordered (achievable with high-precision NTP, 
        GPS clocks, or Lamport clocks)
  iii.  The inputs noted in the log can be reproduced by the simulator (e.g., the
        simulator understands the Quantum API)
  iv.   The log can be interpreted (i.e., we can define a hashmap from
        regex -> simulation method)
  v.    The operators take periodic causally consistent snapshots of the
        running system (since we need a "time zero")

     [Also currently Colin]
