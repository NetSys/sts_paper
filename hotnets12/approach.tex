When the control platform fails to implement the network policy, software
developers need to reproduce the events that triggered the errant behavior.
Yet large networks experience a great number of hardware failures, topology changes,
and other potential triggering events,
all of which may appear characteristic of normal operating
conditions at first glance. In such an environment, identifying relevant
triggering events is tedious at best.

In this section we describe our approach to automated causal
analysis on a log of the system execution. We begin by describing a
generic fault detection algorithm as a prerequisite to causal analysis.
We then describe our algorithm for inferring the set of events in the system
log that are both necessary and sufficient for triggering a given software
fault.

\subsection{Correspondence Checking}

Given a trace of the system execution, our goal is to prune events that are not necessary for triggering
errant behavior. We define errant behavior in terms of network {\em policy}: configuration given to the
control platform describing the desired behavior of the network. Policy
constraints might include connectivity, access control, addressing,
resource allocations, traffic engineering objectives,
or middlebox processing. Errant behavior is then any instance of a
{\em policy-violation}: a configuration of the network that is inconsistent
with the policy in the sense that the disposition of some packets by the
physical forwarding tables conflicts with the dictated policies.

We will need a mechanism for deciding
whether a policy-violation is present at any point in the system execution.
We refer to such an algorithm as a {\em decider}. Deciders take as input a
causally-consistent snapshot of the system state, and return whether the
policy-violation they were designed to detect is present.

We refer to the particular decider we develop here as correspondence checking.
Correspondence checking is generic, in the sense that it does not require
the user to specify invariants. Instead, correspondence checking returns a list
of policy-violations present in the network, and the user checks whether the
policy-violation in mind is present in the list.

In short, correspondence checking provides a crisp determination of all possible packet inputs that
would not behave according to the application's policies if injected into the
network. Correspondence checking relies heavily on the virtual packet algebra pioneered in headerspace
analysis~\cite{hsa}. 
\eat{Furthermore, running correspondence
checking between intermediate layers of the SDN stack (logical view versus
physical view), allow us to identify the component(s) of
the system where policy-violations first manifest themselves.}

Formally, the state of the physical network, the physical view, and the
logical view can be represented as graphs,
$G = (V, E)$. Packets are series of bits, $h \in \{0,1\}^L = H$,
where $L$ is the maximum number of bits in the header.

Upon receiving a packet,
forwarding elements apply a transformation function, potentially modifying
packets before forwarding them on\footnote{Multicast forwarding can expressed
by extending the range to sets of output tuples}:
\begin{align*}
T: (H \times E) \rightarrow (H \times E_{\emptyset})
\end{align*}
Here, $E_{\emptyset} \equiv E \cup \emptyset$, signifying that forwarding elements
may drop packets.

We use $`\Psi`$ to denote the collection of all transfer functions present in
the network at a particular point in time. In this model, network traversal is
simply repeated application of $`\Psi`$.
For example, if a header $h$ enters the network through edge
$e$, its state after $k$ hops will be:
\begin{align*}
\Psi^k(h,e) = \Psi(\Psi(\dots \Psi(h,e)\dots))
\end{align*}

The externally visible behavior of the network can be expressed as the
transitive closure of $\Psi$:
\begin{align*}
\Omega: (H \times E_{access}) \rightarrow (H \times E_{\emptyset}) \\
\Omega(h,e) = \Psi^{\infty}(h,e)
\end{align*}
Here, $E_{access}$ denotes access links adjacent to end-hosts.

The domain of $\Omega^{physical}$ is the set of pairs of end-hosts in the
physical network along with packets that they could possibly produce (before
traversing any interfaces -- although the physical network may encapsulate
packets while in transit, end-hosts do not observe any encapsulation).
We define $\Omega^{view}$ in exactly the same way, where
the logical hosts are abstract representations of the hosts in the physical
network. This definition depends on the observation that end-hosts are represented
in all layers, even if there is not a one-to-one mapping between the
internal vertices of $G^{virtual}$ and $G^{physical}$. We use $f$ to denote
this bijective mapping between physical hosts and logical hosts. As a result of the
bijection $f$, and since $\Omega$ is a total function, the domain
of $\Omega^{physical}$ is bijective with the domain of $\Omega^{view}$.

The range of $\Omega$ is the hypothetical final location and state of the
input packet if it were injected in the network.
Note that packet transformations are reflected in
$\Omega$. For example, if an input packet passes through a NAT,
the source address of the associated output packet will be re-written
appropriately. We also use the special values $DROP$ and $LOOP$ to distinguish
a packet dropped by a network device from a packet entering an
infinite loop (both packet drops and loops stop the packet from ever leaving the network).

In SDN, it should always be the case that:
\begin{align*}
\Omega^{view} \sim \Omega^{physical}
\end{align*}
The equivalence $\sim$ means that the final outcome of any input packet
injected at a host $a$ in the physical network has the same hypothetical final outcome as
the same input injected at $f(a)$, and vice versa.

Correspondence checking takes as input a causally-consistent
snapshot of both the physical network and the
control cluster's state. We discuss in \S\ref{sec:causal_analysis} how this consistent snapshot
is taken. Given a consistent
snapshot, the routing
tables of forwarding elements can be translated into transformation functions.
Finally, we feed a symbolic packet $x^L$ to each access link of the
network.\footnote{The rules for process wildcard bits $x^n$ are defined in
the HSA paper~\cite{hsa}} The end result is a propagation graph representing all possible paths taken by a packet injected
at the access link.

We compute $\Omega$ by traversing the resulting propagation graph. If a packet
is dropped or enters a loop, we mark the value as $DROP$ or $LOOP$. Otherwise, 
the leaves of the propagation graph define the final outcome of the input
packets injected at that access link.

Because network policies are defined by
configuring the logical view, any mismatch between $\Omega^{view}$ and $\Omega^{physical}$
represents an instance of a policy-violation.

\subsubsection{Discussion}

Correspondence checking is a generic detector that does not require the user
to define any invariants. As we will see, this property is important for our
goal of providing as high a degree of automation as possible.

Nonetheless, correspondence checking represents a somewhat weak notion of
correctness. Correspondence checking only captures external behavior and
internal loops; it does not capture internal behavior such as load-balancing
over links. It also assumes that the policies as expressed by the
configuration of the logical view are correct; a user would need to add a more 
stringent invariant checks to detect misspecified policies.
Finally correspondence checking can not verify
time-dependent policies such as ``No link should be congested more than 1\% of
the time'', or ``No server should receive more than 500MB/s of external traffic''.

We leave the design of more sophisticated detectors to future work.
Despite these shortcomings however, we will show in 
\S\ref{sec:evaluation} that correspondence checking is still able of
detecting a class important failure modes.

\colin{TODO: insert Kyriakos' thoughts here}

\subsection{\SIMULATOR{}}
\label{sec:causal_analysis}

\colin{Amin Vahdat:
I understand another key benefit of SDN/OpenFlow is being able to play with a
lot of "what if" scenarios to enable you to fine-tune the network before going
live.

Exactly. So one of the key benefits we have is a very nice emulation and
simulation environment where the exact same control software that would be
running on servers might be controlling a combination of real and emulated
switching devices. And then we can inject a number of failure scenarios under
controlled circumstances to really accelerate our test work.}

\colin{To get a better sense of the inputs to the controller, perhaps we should take
a look at the Quantum plugin API.}


% ============ TERMS ================
{\em External events:} input fed to the control platform triggered by processes outside
of the system. For proactive controllers, external input includes policy
changes, host placement changes, network topology changes (including hardware
failures), and control server failures. Reactive controllers
additionally take traffic changes as input. External events are to be distinguished
from internal events triggered by the system itself, such as message sends or software
crashes.

{\em happens-before:} a transitive relation $\rightarrow$ on the external and internal events of
the system execution, following Lamport's
formulation~\cite{Lamport:1978:TCO:359545.359563}: (i) if $a$ and $b$ are
events in the same process and $a$ comes before $b$, then $a \rightarrow b$,
(ii) if $a$ is a message send by one process and $b$ is the receipt of the
same message by another process, then $a \rightarrow b$. A happens-before
relation defines a partial ordering on the events in the system execution.

{\em Causally-consistent snapshot:} a snapshot of a distributed system's state where
there exists no event $b$ in a node's event history such that $a \rightarrow
b$, but $a$ does not appear in the snapshot.
% ============ /TERMS ================

Correspondence checking infers all policy-violations in the network at a
particular point in time. However, troubleshooters
need two additional pieces of the diagnostic puzzle.

First, in large distributed
systems with communication delay and hardware failures, transient policy-violations
are unavoidable, even common. That is, every time there has been a link failure that the SDN platform has not had time to respond to, or every time there has been a policy change that has not yet propagated down to the physical switches, there is a policy-violation.  Most of these policy violations will be temporary, resolving as soon as the SDN platform has had time to respond.  Some of these policy violations will persist, and that indicates a problem in the SDN platform.  In addition, even some of the ephemeral violations may be harmful (such as those that violate isolation conditions), which again indicate a problem in the SDN platform.

Troubleshooters therefore need a mechanism to differentiate
policy-violations that indicate a problem in the SDN platform from those that merely reflect inherent delays in responding to events; we will call all policy-violations that indicate a problem``pernicious'' violations.

Second, once pernicious policy-violations are encountered, troubleshooters need to
identify the events (link failures, controller failures, VM migrations,
\etc{}) that triggered the problem. Moreover, they would like to have these events narrowed down to a minimal set, to make it easier to understand the problem.

\Simulator{} is our mechanism for providing this information. \Simulator{} models
the network in a single simulation process, thereby providing arbitrary control over hardware
failures, message delays and other failure modes. \Simulator{} distinguishes
between ``internal'' events that happen throughout the normal course of the
system execution (\eg{} message sends and receives), and ``external'' events
injected into the system (\eg{} link failures). We describe the details
of the simulator in the next section.

The key insight behind
\simulator{} is that fault localization is significantly easier with the
ability to selectively filter
out external events from the system execution and observing how the system
plays out in isolation.
With complete control over the
system execution, we are able to programmatically (i) track the lifetime of
policy-violations to differentiate persistent from transient errors, and (ii)
infer the `minimal causal set' of events triggering the problem. We describe
these components below.

Note that we have implemented policy-violation lifetime tracking, but we are
still developing the infrastructure for minimal causal set inference.
Nonetheless, we present our algorithm in this section.

\colin{reviewer A: As for the simulation-based replay analysis, it isn't clear what you
are simulating/emulating and what are the events included in the trace. Please
clarify.} \colin{reviewer D:  This is a nice result, and it
would be nice to see in more detail how this is accomplished.  Is there
an analysis performed on the control program to decide which aspects
of the failure modes of the system are relevant to a problem?
Unfortunately, insufficient information is presented about the model
to indicate that this is true.}

\colin{The Oracle requires a consistent snapshot of both the physical network
and the control cluster's state. In the simulator, this consistent snapshot is
trivial to take: simply pause each controller's VM and the simulation. To
obtain the snapshot that denotes time zero, we assume that the production
network takes periodic snapshots using an algorithm such as Chandy et al's
\cite{Chandy:1985:DSD:214451.214456} consistent snapshot algorithm}

\colin{reviewer C: Does each layer have to output
a transformation
function in a specific format? Does this approach require a
standardization of the interfaces between layers in terms of some
low-level rules? It isn't clear to me that many layers would
interface with a forwarding-table like set of rules.}

\subsubsection{Policy-Violation Lifetime Tracking} The first step in
\simulator{} involves detecting policy-violations and prioritizing them based
on their duration.
We do so in a relatively straightforward fashion. First, we take as input
a stream of network events (\eg{} link failures). Event sequences are either
synthetically generated or gathered from a production trace of failure and topology change
events, as enabled, \eg{}, by OFRewind~\cite{ofrewind} \colin{reviewer A: how is this defined?}.
We then replay the execution of the
control plane based on the input trace. Throughout the system execution,
the simulator periodically invokes correspondence checking to enumerate all
policy-violations (defined as any value in $\Omega^{physical}$ not present in
$\Omega^{virtual}$, or vice versa). When a policy-violation is detected,
the simulator forks off a branch that investigates the future system behavior
in a case where no further failure events are played out. Finally, we
prioritize the policy-violations based on their duration.



\subsubsection{\bf Causal Inference} Having identified persistent
policy-violations, \simulator{} seeks to identify the `minimal causal set' of
events leading up to the problem. By `minimal causal set', we mean
the minimally-sized set of events preceding (defined by the `happens-before'
relation~\cite{Lamport:1978:TCO:359545.359563}) the onset of the policy-violation, such that
if any single event were removed the set, the policy-violation would not have resulted.
% * would not have resulted *in such a long duration event*?

\Simulator{} captures the notion of causality by attaching vector clocks~\cite{Mattern89virtualtime} to
all control messages in the simulated execution of the system. Our
simulator runs as a single sequential thread; therefore we convert any partially-ordered input trace
of network events into a totally-ordered trace by choosing an arbitrary
sequential ordering that maintains the `happens-before'
relation.

Without regarding casual constraints, a naive algorithm to infer the minimal
causal set would be to iteratively
exclude each event from the system execution, and see if the policy-violation
still appears. The problem is that we cannot simply ``erase history'';
antecedent events in the system execution may have depended on the event we
are excluding.

Our algorithm therefore proceeds as as follows. First, consider
the state of the system at exactly the point where the given policy-violation
occurs. The active `causal branches' in the system at this time are (i) the
most recent event (message send, message receive, or internal state change)
occurring on every node in the system, and (ii) the message send event for any
in-flight control packets in the network. Our goal is to prune these causal
branches until we are left with the minimal causal set at the leaves of the
pruned branches.

%\begin{figure}[t]
%    %\hspace{-10pt}
%    \includegraphics[width=3.25in]{../diagrams/approach/localizing}
%    \caption[]{\label{fig:localizing} Localizing the minimal causal sequence for a failure in the
%    event stream}
%\end{figure}

After having identified the active `casual branches' in the system at the
onset of the policy-violation, our algorithm essentially performs a
traversal of the causal graph's (a DAG) topological ordering, pruning leaves that are not
responsible for the policy-violation. In pseudo-code:

\begin{algorithmic}
\State $causalSet \gets []$
\While {$\exists\;\text{\it leaf in causalGraph}$}
    \State {\it remove leaf from causalGraph}
    \State $violation \gets runSimulation(causalGraph)$
    \If {\it not violation}
        \State $causalSet\;+= leaf$
    \EndIf
\EndWhile
\end{algorithmic}

Here, $runSimulation(DAG)$ executes the system from the beginning of the
trace, and checks whether the policy-violation occurs at the end of the
execution. The beginning of the trace starts at some pre-defined point where
the system was known to be in a coalescent state.


\colin{
Panda and I realized that the minimal casual set algorithm we presented in the OSDI submission is erroneous.

A few things are wrong with this:
\begin{itemize}
\item We can't remove elements of the MCS from the DAG! We should only remove from the DAG if the event is not in the MCS; otherwise, the policy-violation will never be reproduced after the first element is added the MCS, since we took away one of the necessary conditions.
\item Need to make a distinction between "random events" (e.g. link failures, node failures) and "deterministic events" (e.g., a node is programmed to send a pong when it receives a ping). In the MCS algorithm, it makes no sense to prune out deterministic events... However, the algorithm still needs to take in the entire casual DAG (both random and deterministic) to maintain casual dependencies [can't prune arbitrary random events]
\item We assume that the random events are independent; we need make this
assumption explicit. But because we assume they are completely independent,
and because we only manipulate random events, there is no need for vector
clocks!!
\end{itemize}
}

\colin{More notes: 
\begin{itemize}
\item How long is the window between snapshot and policy-violation?
\item How often are snapshots taken?
\item What if MCS is empty? $\rightarrow$ increase window
\item Take the description of the log input one step further. Link failures
come from IS-IS. Controller crashes and switch failures come from heartbeats. 
Policy changes are logged as good operating practice. Also, be very careful
about what we mean by non-deterministic event, and acknowledge false
negatives.
\item TODO: need to model policy-changes in the simulator.
\item Vector clocks as an optimization for causal inference. (If concurrent,
can prune automatically)
\item Point out that MCS finds exactly the events that are both sufficient and
necessary.
\item Note that we aren't going after performance problems.
\item We need to know exactly when to inject the non-deterministic events into
the simulated system. Suppose we boot up the system in our simulator starting
from a snapshot. We inject the first non-deterministic event, and the system
reacts by sending out a series of messages. We can't just inject the second
non-deterministic at any point, since our choice of when to interleave the
non-deterministic events amongst the messages might affect the final outcome.
\end{itemize}
}

Note that each iteration of the loop can be performed in parallel by cloning
the state of the simulator. The serial runtime of the algorithm is therefore
linear with the number of events in the input trace.

We also note that Lamport's definition of the `happens-before' relation is
conservatively general, and can include events that are not in fact
casually-related. In fact, as described here, our algorithm can be viewed as a
generalization of dependency graph algorithms for hardware fault
isolation~\cite{577079}. In future work we hope to leverage domain
knowledge of network control plane
protocols to provide a more specific definition of causality (\eg{} OpenFlow
control messages with differing transaction ids are not causally related),
allowing us to further prune events that are not in fact causally related from the DAG.

\subsubsection{Additional Use-Cases} Besides lifetime tracking and causal analysis, our simulation infrastructure has a
number of other possible use-cases:

\colin{Make sure we describe how the policy-violation of interested is
originally identified by the operator/developer. Also don't forget to
emphasize the distinction between transient and persistent}

\noindent\textbf{Checking related problems by fuzzing.} Input traces can be \emph{fuzzed}, i.e.,
randomly perturbed, to expose the system to similar error conditions, and confirm
that a proposed solution is not just a point-fix. \colin{reviewer A: Fuzzing
can be applied across a multitude of dimensions. What kind of fuzzing do you
suggest using?}

\noindent\textbf{Investigating pathological environment conditions.} The simulator allows for investigation
of pathological environment conditions difficult to achieve in a real world test bed
(\eg{}, correlated failure rates, extremely long delays etc.). This enables
investigation of situations that have a high potential for triggering errors.

\noindent\textbf{Interactive exploration.} Troubleshooters can also interactively bisect
the trace or modify specific events to further pinpoint the cause for a failure.
This is useful as soon as a suspect event sequence has been identified.

\noindent\textbf{Regression/Integration Test Library.} In traditional software engineering practices,
integration tests are an
important part of the software development cycle: developers feed end-to-end
input through the system, and verify that the system execution satisfies
certain safety and liveness properties. As additional failure cases are encountered in
production, new cases can be added to a suite of integration tests to
ensure robust operation of the system in future versions of the system.

Although the practice of accumulating an integration test suite over time is
commonplace in other fields of computer science, the field of networking
simply did not have the requisite software infrastructure to realize this practice before the emergence
of SDN. \Simulator{} can be viewed as our realization
of this development practice, applied to network controllers. Our simulator's fine-grained control over
failure scenarios allows us to test corner-case network conditions -- those
that are most difficult to anticipate in traditional unit tests.
As known failure cases are accrued over time, we envision \simulator{} being used to validate
new and existing SDN platforms.

\subsection{Discussion}

Correspondence checking and \simulator{} serve to isolate the platform layer and
event sequence responsible for a given error. \projectname{} can be
complemented by classical debugging techniques (\eg{} log messages and source
code debugging) to identify the root cause of
the failure in the code. These techniques are much more
effective when applied a specific event sequence. Once a
potential fix has been developed, it can be validated by repeating the
problematic execution within \projectname{}. Input fuzzing further helps to
validate whether there are
related error events that the patch missed.

\colin{TODO: paragraph on how this approach might apply to traditional
networks}
