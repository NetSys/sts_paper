\colin{Should cite Interactive Debugging for ISPs~\cite{lin2009towards}.}

Our work spans three fields: systems and networking, software engineering, and
programming languages.

\noindent{\bf Systems and Networking.}
We share the common goal of improving troubleshooting
of software-defined networks with OFRewind~\cite{ofrewind} and
ndb~\cite{handigol2012debugger}. OFRewind provides
record and replay of OpenFlow control channels, and
ndb provides a
trace view into the OpenFlow forwarding tables
encountered by packets in the network.
Neither ndb nor OFRewind address the problem of diagnostic information
overload: with millions of packets on the wire, it can
be challenging to pick just the right subset to interactively debug.
To the
best of our knowledge, \projectname~is the first system that programmatically
isolates the inputs that caused the network to enter an invalid
configuration in the first place.

\colin{
Runtime invariant checking.
WiDS checker introduced the notion of recording
production executions to be later replayed and verified in a controlled simulation~\cite{Liu07widschecker:}.
D3S made this invariant checking process realtime.
}

\eat{
Trace analysis frameworks such as Pip~\cite{pip} allow developers
to programmatically check whether their expectations about the structure of
recorded causal
traces hold. MagPie~\cite{barham2004using} automatically identifies anomalous
traces, as well as unlikely transitions within anomalous traces by constructing
a probabilistic state machine from a large collection of traces and
identifying low probability paths.
Our approach identifies the exact minimal causal set of inputs without
depending on probabilistic models.

Network simulators such as
Mininet~\cite{handigol2012reproducible}, ns-3~\cite{ns3}, and ModelNet~\cite{Vahdat:2002:SAL:844128.844154}
are used to prototype and test network software.
Our focus on comparing diverged histories requires us
to provide precise replay of event sequences, which is in tension with the performance
fidelity goals of pre-existing simulators.

% Could remove dependency inference citation if strapped for space
Root cause analysis~\cite{yemini1996} and dependency inference~\cite{Kandula:2009:DDE:1592568.1592597}
techniques seek to identify the minimum set of failed
components (\eg~link failures) needed to explain a collection of alarms. Rather than
focusing on individual component failures, we seek to minimize inputs that affect the behavior
of the overall distributed system.
}

\noindent{\bf Software Engineering \& Programming Languages}
Sherlog~\cite{Yuan:2010:SED:1736020.1736038} takes on-site logs from a
single program that ended in a failure as input, and applies static analysis to infer the
program execution (both code paths and data values) that lead up to the failure.
%Along a similar vein, execution
%synthesis~\cite{Zamfir:2010:EST:1755913.1755946} takes a program and a bug
%report as input, and employs symbolic execution to find a thread schedule that will
%reproduce the failure.
The authors of delta debugging
also applied their technique to multi-threaded (single-core) programs
to identify the minimum set of thread
switches from a thread schedule (a single input file) that reproduces
a race condition~\cite{choi2002isolating}.
Chronus presents a simpler search
algorithm than delta debugging that is specific to configuration
debugging~\cite{whitaker2004configuration}.
These and many other similar techniques focus on troubleshooting single, non-distributed
systems.

%Rx~\cite{qin2005rx} is a technique for improving availability: upon
%encountering a crash, it starts from a previous checkpoint, fuzzes
%the environment (\eg~random number generator seeds) to avoid triggering the same bug,
%and restarts the program. Our
%approach perturbs the inputs rather than the environment
%prior to a failure.

Model checkers such as Mace~\cite{Killian:2007:MLS:1250734.1250755} and
NICE~\cite{nice} enumerate all possible code paths taken by control software (NOX)
and identify concrete inputs that cause
the system to enter invalid configurations. Model checking works well for small
control programs and a small number of machines, but suffers from exponential
state explosion when run on large systems. For example, NICE took 30 hours to
model check a network with two switches, two hosts, the MAC-learning
control program (98 LoC), and five concurrent
messages between the hosts~\cite{nice}. Rather than exploring all
possibilities, we discover bugs through testing and systematically
enumerate subsequences of their event traces
in polynomial time.

