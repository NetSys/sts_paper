In this section we provide an overview of the SDN architecture and the types
of errors observed in production software-defined networks.

\subsection{SDN Architecture}

Modern SDN platforms are highly complex. We enumerate the characteristics of
these systems here.

In order to scale to large networks and achieve fault tolerance for the control
plane itself, SDN platforms distribute control state across cluster(s) of servers.
Onix~\cite{onix}, for example,
partitions a graph of the network state across either an eventually-consistent
DHT or a transactional database. Control applications can thus make their own
tradeoffs in choosing consistency models, degree of
fault tolerance, and other properties.

The modern SDN stack contains up to three layers: the lowest
layer is responsible for tracking switch state and pushing configuration
changes to the switches themselves; a virtualization layer
wraps the state of the network in a programmable abstraction; and a control
application written by network administrators 
expresses the network policy in terms of the virtualization layer abstraction.
A depiction of this architecture is shown in Figure \ref{fig:basicarch}.

The virtualization layer facilitates a concise specification of
intended network behavior. A common pattern is to represent an entire
datacenter network as a single logical switch. In this manner, operators
can specify routing, access control, and QoS policies by configuring a single forwarding
device; the platform then maps the configuration onto sequences 
of forwarding elements in the physical network.

Virtualization additionaly facilitates multi-tenancy, and isolates control applications from the specifics
of the underlying network. Ideally, each application is reduced to a
state-less, side-effect free function $f(view) \rightarrow configuration$.

\begin{figure}[t]
    %\hspace{-10pt}
    \includegraphics[width=3.25in]{../diagrams/architecture/SDN_stack.pdf}
    \caption[]{\label{fig:basicarch} Depiction of the layered SDN stack.} 
\end{figure}

SDN controllers can be further categorized according to their flow
installation model: proactive or reactive.
Proactive controllers pre-compute forwarding tables for the entire network,
and only push down updates periodically to react to link failures, changes in
traffic mix, \etc. In contrast, reactive controllers forward all new flows to
control servers. After a control decision is made, a flow entry is installed
into the ingress switch, and the packet is forwarded along.
\colin{Mention that we focus on proactive? And that industry does too? :P}

\subsection{Platform Failure Modes}

Here we provide a taxonomy of common failure modes observed in
the SDN architecture.

\begin{itemize}
\item Taxonomy of failure modes introduced by these abstractions (Google Doc),
and why they're hard to debug.
\item Fence-post: we focus on platform bugs. This is complementary to work on
application debugging (NICE) and dataplane debugging (Anteater, HSA).
\end{itemize}

\colin{The following snippet from our class report is far too verbose, and does not reflect any of our
latest thinking from the google doc.}

Correspondence errors are particularly pernicious, as the 
virtualization translation layer of the platform is often quite complex. 

Production SDN deployments distribute the NOS  across multiple servers to achieve fault-tolerance
and scalability. Distributed control gives rise to the same classes of coordination
and consistency bugs that affect general distributed systems. 

We note that two distinct classes of consistency violations occur in SDNs.
Transient consistency violations depend on the ordering of events in the system. For example,
if the  NOS pushes out a new forwarding policy
which involves multiple switches, in-flight packets may enter a
temporary forwarding loop while each of the switches converge on the new policy.
The effects of transient
consistency violations are noticeably distinct from persistent violations.
