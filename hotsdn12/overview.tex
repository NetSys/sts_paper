In this section we provide an overview of the SDN architecture and the types
of errors observed in production software-defined networks.

\subsection{SDN Architecture}

Modern SDN platforms are highly complex. We enumerate the characteristics of
these systems here.

In order to scale to large networks and achieve fault tolerance for the control
plane itself, SDN platforms distribute control state across cluster(s) of servers.
Onix~\cite{onix}, for example,
partitions a graph of the network state across either an eventually-consistent
DHT or a transactional database. Control applications can then make their own
tradeoffs between consistency and latency, level of fault tolerance, and other
properties.

Modern SDN controllers are architected with a three layer approach: the lowest
layer is responsible for tracking switch state and pushing configuration
changes to the switches themselves; a virtualization layer
wraps the state of the network in a programmable abstraction; and a control
application written by network administrators 
expresses the network policy in terms of the virtualization layer abstraction.
A depiction of this architecture is shown in Figure \ref{fig:basicarch}.
This facilitates a concise specification of
intended network behavior, and isolates control applications from the specifics
of the underlying network. Ideally, each application is reduced to a
state-less, side-effect free function $f(view) \rightarrow configuration$,
which is
easy to validate. \colin{Mention multi-tenancy?}

\begin{figure}[t]
    %\hspace{-10pt}
    \includegraphics[width=3.25in]{../diagrams/architecture/SDN_stack.pdf}
    \caption[]{\label{fig:basicarch} Depiction of the layered SDN stack.} 
\end{figure}

SDN controllers can be further categorized according to their flow
installation model: proactive or reactive.
Proactive controllers pre-compute forwarding tables for the entire network,
and only push down updates periodically to react to link failures, changes in
traffic mix, \etc. In contrast, reactive controllers forward all new flows to
control servers. After a control decision is made, a flow entry is installed
into the ingress switch, and the packet is forwarded along.
\colin{Mention that we focus on proactive? And that industry does too? :P}

\subsection{Platform Failure Modes}

\begin{itemize}
\item Taxonomy of failure modes introduced by these abstractions (Google Doc),
and why they're hard to debug.
\item Fence-post: we focus on platform bugs. This is complementary to work on
application debugging (NICE) and dataplane debugging (Anteater, HSA).
\end{itemize}
