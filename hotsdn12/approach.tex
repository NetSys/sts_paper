In this section we present two mechanisms to facilitate the troubleshooting
process. Correspondence checking allows troubleshooters to isolate
the cause of policy-violations to a particular layer. \Simulator{}
allows troubleshooters to isolate relevant events throughout the system
execution. We provide the details of these techniques below. 

\subsection{Correspondence Checking}

The sole task of the application layer is to specify network
policies. The platform translates these high-level policies
into changes in the physical network.

Platform correctness can be expressed as a simple invariant:
the state of the application layer should correspond to the state of the
physical network. We check this invariant by applying the virtual packet
algebra pioneered in headerspace analysis~\cite{hsa}. 

Formally, each layer of the SDN stack can be represented as a graph,
$G = (V, E)$. Packets are series of bits, $h \in \{0,1\}^L = H$,
where $L$ is the maximum number of bits in the header. Upon receiving a packet,
forwarding elements apply a transformation function, potentially modifying
packets before forwarding them on\footnote{We assume unicast forwarding for
clarity}:
\begin{align*}
T: (H \times E) \rightarrow (H \times E_{\emptyset})
\end{align*}

We use $`\Psi`$ to denote the collection of all transfer functions present in
the network at a particular point in time. In this model, network traversal is simply a composition of transformation
functions. For example, if a header $h$ enters the network through edge
$e$, its state after $k$ hops will be:
\begin{align*}
\Phi^k(h,e) = \Psi(\Psi(\dots \Psi(h,e)\dots))
\end{align*}

The externally visible behavior of the network can be expressed as the
transitive closure of $\Phi$:
\begin{align*}
\Omega: (H \times E_{access}) \rightarrow (H \times E_{\emptyset}) \\
\Omega(h,e) = \Phi^{\infty}(h,e)
\end{align*}
Here, $E_{access}$ represents access links adjacent to end-hosts.

In SDN, it should always be the case that 
$\Omega^{view} \sim \Omega^{physical}$. Informally, this means that
any packet injected at an access link in $G^{virtual}$ should arrive at
the same final location as the corresponding (encapsulated) packet injected at the
corresponding access link in $G^{physical}$. Note that hosts are represented
in all graphs, although though there may not be a one-to-one mapping between the
internal vertices of $G^{virtual}$ and $G^{physical}$.

To check correspondence in SDN, we begin by taking a causally consistent
snapshot~\cite{Chandy:1985:DSD:214451.214456} of the physical network. The routing
tables of forwarding elements can then be translated into transformation functions.
Finally, we feed a symbolic packet $x^L$ to each access link of the
network~\footnote{The rules for process wildcard bits $x^n$ are defined in
the HSA paper~\cite{hsa}}. The end result is a propagation graph representing all possible paths taken by a packet injected
at the access link.

The leaves of the propagation graph represent $\Omega$. We
verify correspondence in SDN by generating propagation graphs for all SDN layers,
and comparing the leaves.

\subsection{\SIMULATOR{}}

Correspondence checking allows to detect inconsistencies between the layers and
policy violations present in the network at a particular point in time, and
localizes the error to a particular layer in the SDN stack. However, it only
captures a snapshot, and does not enable the troubleshooter to classify the
gravity of the detected inconsistencies, i.e., whether the detected 
inconsistencies are \emph{harmless} and temporary, i.e., byproducts of the forwarding reconverging 
after a link failure, or whether they are \emph{pernicious} and persistent.
Also, it cannot aid in isolating the precise \emph{even sequence} that triggered
the faulty behavior.

To achieve these goals, we complement correspondence checking with
\emph{simulation-based replay analysis}. We base our replay on a consistent
trace of low level SDN events, as enabled, e.g., by OFRewind~\ref{ofrewind}. The
events from the trace are fed into a simulator that invokes the correspondence
checking at every time step. The simulator focuses on the \emph{sdn control plane},
and models the failure modes in sufficient detail to reproduce the error, while
allowing for complete control of the timing, ordering, and production of the events.

\textbf{Identifying pernicious inconsistencies}. When an inconsistency is detected,
the simulators forks off a simulation branch that investigates the future system behavior
in a case where \emph{no further external events are received}. If the detected inconsistency
is resolved in isolation within a customizable number of simulation time steps, it is considered
a temporary problem. If it is not, this is a strong indication that the system has indeed
reached a persistently inconsistent state that needs to be investigated.

\textbf{Checking related problems by fuzzing} Input traces can be \emph{fuzzed}, i.e.,
randomly pertubed, to expose the system to similar error conditions, and confirm
that a proposed solution is not just a point-fix.

\textbf{Investigating pathological environment conditions} The simulator allows for the investigation
of pathological environment conditions difficult to achieve in a real world test bed
(i.e., high, correlated failure rates, extremely long delays etc.). This enables
investigation of situations that have a high failure potential.

\textbf{Interactive exploration} Troubleshooters can also interactively bisect
the trace or modify specific events to further pinpoint the cause for a failure.
This is useful as soon as a concrete suspicious event sequence has been identified.

\subsection{Discussion}
Correspondence checking and \simulator{} serve to isolate the systems layer and
the event sequence responsible for a given error. To identify the root cause of
the failure in the code, they can be complemented by classical debugging
techniques,~i.e., log messages and source code debugging. These are much more
effective when applied to investigate a specific event sequence. Once a
potential fix has been developed, it can be validated by repeating the
problematic replay. Fuzzing helps to validate whether there may be
related error events that the patch may have left open.


