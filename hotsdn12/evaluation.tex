Two candidates for root cause: 
\begin{itemize}
\item scale-out BGP router processes a BGP advertisement improperly. Turns out
to be a controller failure $\rightarrow$ improper failover (out of date NOM).
\item Isolation Brach: Storage servers aren't supposed to be reachable from the Internet, only
internal machines (like EC2). router fails, failover doesn't have proper ACLs, operator
observes traffic increases, observes increased traffic showing up at servers. 
\end{itemize}

\begin{itemize}
\item Use case 1: network operator doesn't know anything about SDN.
    \begin{itemize}
    \item Operator pays for scale-out BGP router. 
    \item A particular BGP update fails to be installed -- blackhole
    \item Runs correspondence checking, and there are indeed inconsistencies!
    \item Call up Nicira!
    \end{itemize}
\item Use case 2: SDN developer deals with that customer complaint
    \begin{itemize}
    \item feeds in current production snapshot to simulator, verifies
    blackhole 
    \item backs up to state before BGP update
    \item Operator steps through execution until BGP update occurs
    \item sees that there was a switch failure
    \item Isolates the events that matter.
    \item Plays them forward to differentiate ephemeral from persistent.
    \item Puts in log statements, re-runs through debugger
    \item eventually finds one line of code that caused the problem.
    \item pushes change to production
    \end{itemize}
\item Potential use case 3: library of corner cases as integration test /
burn-in test:
    \begin{itemize}
    \item Simulator gives complete control over network
    \item Unlike code tests, which exercise code branches, simulator exercises
    the network!
    \item Gather library of corner-cases that have caused other controllers to
    topple
    \end{itemize}
\end{itemize}

\subsection{Overhead}

\noindent{\bf Simulator Scalability.} We have implemented a prototype
simulator. In a single process, our prototype models hosts and OpenFlow switches 
as python objects, which communicate with a SDN controller as if they were
true hardware.

To gauge the scalability of modeling the network within a single-process,
we \colin{will have} spawned a network of 10,000 switches and measured the
time to process 5 FLOW\_MOD messages per switch. \colin{It took X seconds,
which is below the bounds of human perception}

%\tbd{Graph of simulator
%scalability. x-axis is \# of software switches. y-axis is time to process N
%OFTP\_FLOW\_MOD messages. A few lines for different values of N} \\

\noindent{\bf Record and Replay Overhead.} In contrast to general record-and-replay
mechanisms, the amount of recorded state needed for
high-fidelity replay is tractable. With proactive flow installation, 
updates are pushed to routing tables over a relatively long time scale; periodic
FIB snapshots along with a log of link state events, control server
downtime, and host mobility information suffice for our purposes. As a point of reference, the Cisco 7000 series
core switch model supports a maximum of 128K MAC entries and
128K ACL entries~\cite{cisco7000}. Assuming 36 bytes per flow entry,
(larger than OpenFlow 13-tuple), this yields
an upper bound of 9216 bytes, uncompressed, per FIB. A datacenter of 100,000
hosts includes roughly 8,000 switches~\cite{Al-Fares:2008:SCD:1402946.1402967}.
Therefore a snapshot of the FIBs of the entire network takes up roughly 74 MB.
The VL2 paper reports 36M network error events over one year over 8
datacenters, which implies 8.5 error events per minute per
datacenter~\cite{Greenberg:2009:VSF:1592568.1592576}.
Suppose we took a snapshot of the FIBs in the network every second. 
We would need then to store roughly 4GB uncompressed per minute, a relatively small growth 
rate for datacenter logs. Note that this is a conservative overestimate.
\tbd{Get numbers on frequency of VM migrations}
%To account for host mobility, assume that each server hosts 10 VMs,
%and 1\% of VMs are created, suspended, or migrated every minute. Then 10,000 host mobility events must be
%logged per minute, also a reasonable storage cost. \colin{get real numbers}

%As a point of reference, border routers' working RIB size is
%$\textasciitilde$130MB~\cite{Karpilovsky:2006:UFR:1368436.1368439}.

\noindent{\bf Correspondence Checking Runtime.} Correspondence checking has
the same computational complexity as the loop detection algorithm described in
HSA~\cite{hsa}: \\
$O(c^{d}dR^{2}P)$, where $c$ is the fragmentation factor (a
small constant close to $1$), $d$ is the diameter of the network, $R$ is
the number of switches in the network, and $P$ is the number of access links
in the network. The propogation graph for each access link can be computed
in parallel, so the remaining serial portion of the runtime is $O(c^{d}dR^{2})$.

To test the runtime of correspondence checking in practice, we \colin{will
have} ran the algorithm for different network sizes. The results are shown in
Figure~\ref{fig:hsa_runtime}. We see that \colin{foobar}.

\begin{figure}[t]
    %\hspace{-10pt}
    \includegraphics[width=3.25in]{../graphs/mock_hsa_overhead.pdf}
    \caption[]{\label{fig:hsa_runtime} Runtime of correspondence checking
    versus size of the network}
\end{figure}

\noindent{\bf Miscellaneous} \colin{How do you validate the simulator? Is it sufficiently complex to
capture real-world bugs? Conversely, if I catch a failure mode in the
simulator, will my fix work in the production network?}
